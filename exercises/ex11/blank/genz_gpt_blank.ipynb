{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-gz",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 for Gen-Z Style Rewriting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d15907",
   "metadata": {},
   "source": [
    "- We will try to fine-tune GPT-2 to rewrite formal text into Gen Z style.\n",
    "- For this, we will be using something called **knowledge distillation**.\n",
    "- What is knowledge distillation, you might ask?\n",
    "  - It's when you train / fine-tune a smaller model on a bigger model’s outputs.\n",
    "  - The big model is called the **teacher model**, and the small model is, well, you probably guessed it, the **student model**.\n",
    "- So how exactly will we be doing that?\n",
    "  - We will be generating synthetic data, some call this process **data augmentation**, using the Magistral Small model.\n",
    "    - The script for this is provided: `get_pairs_magistral`. You can take a look, but Magistral Small requires a powerful GPU to run. You might be able to run it with an RTX 4090 (if quantized), but I would assume most of you don’t have this kind of compute power available, so you can simply look at the script, it's very straight to the point.\n",
    "    - The output of Magistral Small is pairs of formal text and Gen Z text.\n",
    "  - We will use the output of Magistral Small throughout the exercise. The file that contains our pairs is `genz_pairs_magistral_diverse_5k.jsonl`.  \n",
    "    (A small note if you’re interested: JSONL is used instead of JSON because JSONL supports streaming the inference output directly, so we don’t have to wait for the whole pipeline to end to get our data.)\n",
    "- HAVE FUN!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "# some colors to add some soul\n",
    "RED = \"\\033[91m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load JSONL Style-Transfer Dataset\n",
    "\n",
    "Your dataset contains pairs:\n",
    "```\n",
    "{\"original\": \"I am tired\", \"gen_z\": \"lowkey exhausted fr\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jsonl-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = \"genz_pairs_magistral_diverse_5k.jsonl\"\n",
    "\n",
    "df = pd.read_json(jsonl_path, lines=True)\n",
    "print(\"Dataset size:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split",
   "metadata": {},
   "source": [
    "## Train / Validation Split\n",
    "- Tasks: usually we create 3 splits: train, validation, test. But this task is more about creativity, there are methods where we can test our output for stylistic rewriting, but the pairs themselves were generated using another LLM, so I think after we finish fine-tuning our mode, judge it yourself!\n",
    "    - Task 1: 80% train + 20% dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    # --- Task 1 begins here ---\n",
    "    'train': \"NOT IMPLEMENTED YET\",\n",
    "    'validation': \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 1 ends here ---\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer",
   "metadata": {},
   "source": [
    "## Tokenizer & Pair Formatting\n",
    "- Explanation of the next steps:\n",
    "    - GPT-2 is a causal language model. That means it learns by predicting the next token given everything before it.\n",
    "        - So during training, GPT-2 assumes:\n",
    "            - Every token in the input is something it should try to predict.\n",
    "            - But in our task, the input contains two parts:\n",
    "                -  `Original: <formal text> Gen Z:`\n",
    "            - Our target output is this:\n",
    "                - `<gen-z rewrite>`\n",
    "            - We do not want GPT-2 to learn the prompt, only the Gen-Z rewrite.\n",
    "            - That’s why we need to:\n",
    "                - concatenate prompt + output into one sequence\n",
    "                - but mask the prompt labels with -100\n",
    "\n",
    "- Tasks: let's create our tokenization function\n",
    "    - Task 1: format your prompt + target\n",
    "    - Task 2: tokenize?\n",
    "    - Task 3: build your model input (truncation for safety!) + your labels (remember we don't want our model to learn the prompt, this is very essential for style transfer. Hint: -100 for masking)\n",
    "    - Task 4: tokenize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt_template = \"Original: {src}\\nGen Z: \"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "def tokenize_pair(example):\n",
    "    # --- Task 1 begins here ---\n",
    "    prompt = \"NOT IMPLEMENTED YET\"\n",
    "    target = \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 1 ends here ---\n",
    "\n",
    "    # --- Task 2 begins here ---\n",
    "    prompt_ids = \"NOT IMPLEMENTED YET\"\n",
    "    target_ids = \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 2 ends here ---\n",
    "    \n",
    "    # --- Task 3 begins here ---\n",
    "    input_ids = \"NOT IMPLEMENTED YET\"\n",
    "    labels = \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 3 ends here ---\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# --- Task 4 begins here ---\n",
    "tokenized_datasets = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 4 ends here ---\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['original','gen_z'])\n",
    "tokenized_datasets.set_format('torch')\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collator-expl",
   "metadata": {},
   "source": [
    "## Data Collator: Dynamic Padding and Masking\n",
    "\n",
    "- We want our batches to look like this at training time:\n",
    "```txt\n",
    "[input_ids]      → padded to max seq len in batch\n",
    "\n",
    "[labels]         → padded, but masked with -100\n",
    "\n",
    "[attention_mask] → tells the model which tokens are padding\n",
    "```\n",
    "- Unlike tokenization (which truncates), batching must:\n",
    "    - Dynamically pad sequences in each batch\n",
    "    - Mask padded label tokens with -100\n",
    "    - Ensure padding does NOT affect loss\n",
    "    - Produce a valid attention_mask\n",
    "- Tasks:\n",
    "    - Task 1: store the tokenizer inside the collator\n",
    "    - Task 2: extract tensors from features (input_ids, labels)\n",
    "    - Task 3: dynamically pad inputs (Pad with pad_token_id)\n",
    "    - Task 4: dynamically pad labels (Pad with -100 — so loss ignores padding)\n",
    "    - Task 5: build attention mask (1 for tokens, 0 for padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collator-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalGenZCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        # --- Task 1 begins here ---\n",
    "        pass\n",
    "        # --- Task 1 ends here ---\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # --- Task 2 begins here ---\n",
    "        input_ids  = \"NOT IMPLEMENTED YET\"\n",
    "        labels     = \"NOT IMPLEMENTED YET\"\n",
    "        # --- Task 2 ends here ---\n",
    "        \n",
    "        # --- Task 3 begins here ---\n",
    "        input_ids = pad_sequence(\n",
    "            \n",
    "        )\n",
    "        # --- Task 3 ends here ---\n",
    "\n",
    "        labels = pad_sequence(\n",
    "            \n",
    "        )\n",
    "        # --- Task 4 begins here ---\n",
    "        \n",
    "        # --- Task 5 begins here ---\n",
    "        attention_mask = \"NOT IMPLEMENTED YET\"\n",
    "        # --- Task 5 ends here ---\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "data_collator = CausalGenZCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-expl",
   "metadata": {},
   "source": [
    "## Baseline GPT-2 (No Fine-tuning)\n",
    "\n",
    "- Before we fine-tune GPT-2, we want to know:\n",
    "    - How well does the pretrained model already perform on our Gen-Z rewriting task?\n",
    "- We’ll evaluate GPT-2 without training to compute:\n",
    "```txt\n",
    "baseline loss  → cross-entropy on validation set\n",
    "baseline ppl   → perplexity (exp(loss))\n",
    "```\n",
    "- This gives us a reference score so we can measure:\n",
    "    - how much fine-tuning improves the model\n",
    "    - whether our training setup actually helps\n",
    "    - whether we’re overfitting or degrading performance\n",
    "- We did this in previous exercies, before attempting to change things, we need to set a baseline!\n",
    "\n",
    "- Tasks:\n",
    "    - Task 1: load the `gpt2`\n",
    "    - Task 2: define evaluation-only training arguments (let's go with `batch_size = 4`)\n",
    "    - Task 3: create a Trainer for evaluation\n",
    "    - Task 4: run evaluation on the validation split\n",
    "    - Task 5: calculate the perplexity from loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1 begins here ---\n",
    "baseline_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "# --- Task 1 ends here ---\n",
    "\n",
    "baseline_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# --- Task 2 begins here ---\n",
    "baseline_args = TrainingArguments(\n",
    "    \n",
    ")\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "# --- Task 3 begins here ---\n",
    "baseline_trainer = Trainer(\n",
    "    \n",
    ")\n",
    "# --- Task 3 ends here ---\n",
    "\n",
    "# --- Task 4 begins here ---\n",
    "baseline_eval = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 4 ends here ---\n",
    "\n",
    "# --- Task 5 begins here ---\n",
    "baseline_loss = \"NOT IMPLEMENTED YET\"\n",
    "baseline_ppl = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 5 ends here ---\n",
    "\n",
    "print(f\"Baseline perplexity: {baseline_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-expl",
   "metadata": {},
   "source": [
    "## Fine-tune GPT-2 on Gen-Z pairs\n",
    "- A quick recap of our training objective:\n",
    "```txt\n",
    "Input  → \"Original: <text>\"\n",
    "Output → \"Gen Z: <rewritten text>\"\n",
    "```\n",
    "- Tasks:\n",
    "    - Task 1: define our training arguments (`batch_size = 4`)\n",
    "    - Task 2: initialize the Trainer\n",
    "    - Task 3: let's fine-tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# --- Task 1 begins here ---\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    ")\n",
    "# --- Task 1 ends here ---\n",
    "\n",
    "# --- Task 2 begins here --- \n",
    "trainer = Trainer(\n",
    "    \n",
    ")\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "# --- Task 3 begins here ---\n",
    "\n",
    "\n",
    "# --- Task 3 ends here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-expl",
   "metadata": {},
   "source": [
    "## Evaluation After Fine-tuning\n",
    "- Now that we fine-tuned our model, let's compare the perplexity before and after fine-tuning\n",
    "- Tasks:\n",
    "    - Task 1: get the evaluation results\n",
    "    - Task 2: compute the perplexity after fine-tuning\n",
    "    - Task 3: plot a simple bar chart to visualize the difference (use `pyplot`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1 begins here ---\n",
    "eval_results = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 1 ends here ---\n",
    "\n",
    "# --- Task 2 begins here ---\n",
    "eval_loss = \"NOT IMPLEMENTED YET\"\n",
    "eval_ppl = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "print(f\"Perplexity after fine-tuning: {eval_ppl:.2f}\")\n",
    "\n",
    "metrics = [\"Baseline\", \"Fine-tuned\"]\n",
    "values = [baseline_ppl, eval_ppl]\n",
    "\n",
    "# --- Task 3 begins here ---\n",
    "\n",
    "# --- Task 3 ends here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gen-demo",
   "metadata": {},
   "source": [
    "## Try Gen-Z Rewrite Generation\n",
    "- Now let's see how's our small fine-tuned model doing\n",
    "- You have written functions to run inference before, so this time I'll give it to you on a silver platter :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gen-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_gen_z(text):\n",
    "    prompt = prompt_template.format(src=text)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=400,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Gen Z:\")[-1].strip()\n",
    "\n",
    "sample = \"I've been trying to balance my work, studies, and social life lately, but it feels like everything keeps happening at once. I’m grateful for the opportunities I have, yet I can’t deny that the constant pressure is starting to wear me down. Sometimes I just want a quiet evening to disconnect from notifications, clear my thoughts, and remind myself that I don’t have to keep up with everyone all the time.\"\n",
    "original = textwrap.fill(sample, width=76)\n",
    "gen_z = textwrap.fill(rewrite_gen_z(sample), width=76)\n",
    "\n",
    "print(f\"{BOLD}{RED}Original {RESET}\")\n",
    "print(\"=\" * 80)\n",
    "print(original)\n",
    "print(f\"{BOLD}{BLUE}Gen-Z {RESET}\")\n",
    "print(\"=\" * 80)\n",
    "print(gen_z)\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
