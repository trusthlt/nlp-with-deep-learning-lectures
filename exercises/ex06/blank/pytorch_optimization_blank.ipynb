{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# Fun Stuff with PyTorch!\n",
    "\n",
    "- In this notebook, we’ll be going through 4 iterations of the same problem, each using a different approach.\n",
    "\n",
    "- We’re working with a 3rd-order Taylor polynomial approximation of the function $y = \\sin(x)$, which looks like $y \\approx a + bx + cx^2 + dx^3$.\n",
    "\n",
    "- The true coefficients of this approximation are roughly: $a \\approx 0$, $b \\approx 1$, $c \\approx 0$, $d \\approx -0.166$\n",
    "\n",
    "- **Note:** each version will include an animation at the end showing how our model updates the coefficients over time (as it minimizes the loss).\n",
    "\n",
    "  - And don’t worry about the long code blocks, a big chunk of the code is just for animation. You’ll see comments marking where that part begins and ends.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hey, don't forget to import our libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "# random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version1-intro"
   },
   "source": [
    "## Version 1: Manual Gradients\n",
    "\n",
    "- Task 1: Initialize our random weights\n",
    "- Task 2: Implement the forward Pass (Hint: what exactly are we trying to predict?)\n",
    "- Task 3: Loss Calculation (Hint: what’s the usual way to measure the error between the true value and the predicted value?)\n",
    "- Task 4: Implement backpropagation (You have to do it manually!)\n",
    "- Task 5: Update the weights (Hint: these are the ones you initialized back in Task 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "version1-run"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Here we create our input and output data\n",
    "    x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "    y = torch.sin(x)\n",
    "\n",
    "\n",
    "    # --- Your implementation starts here ---\n",
    "    \n",
    "    # Task 1 --- BEGIN ---\n",
    "\n",
    "    # Task 1 --- END ---\n",
    "\n",
    "    # Don't Remove ME! (Store history for animation) --- BEGIN ---\n",
    "    history = []\n",
    "    save_every = 10\n",
    "    # Don't Remove ME! (Store history for animation) --- END ---\n",
    "\n",
    "    # Here we pick our learning rate\n",
    "    learning_rate = 1e-6\n",
    "    \n",
    "    for t in range(2000):\n",
    "        \n",
    "        # Task 2 --- BEGIN ---\n",
    "        y_prediction = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 2 --- END ---\n",
    "\n",
    "\n",
    "        # Task 3 --- BEGIN ---\n",
    "        loss = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 3 --- END ---\n",
    "        \n",
    "        # Don't Remove ME! (Store history for animation) --- BEGIN ---\n",
    "        if t % save_every == 0:\n",
    "            history.append({\n",
    "                'iteration': t,\n",
    "                'y_pred': y_prediction.detach().clone(),\n",
    "                'loss': loss.item(),\n",
    "                'params': (a.item(), b.item(), c.item(), d.item())\n",
    "            })\n",
    "\n",
    "        if t % 100 == 99:\n",
    "            print(t, loss.item())\n",
    "        # Don't Remove ME! (Store history for animation) --- END ---\n",
    "\n",
    "        # Task 4 --- BEGIN ---\n",
    "        \n",
    "        # Task 4 --- END ---\n",
    "        \n",
    "        # Task 5 --- BEGIN ---\n",
    "        \n",
    "        # Task 5 --- END ---\n",
    "        \n",
    "    print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
    "\n",
    "\n",
    "    # --- Your implementation ends here ---\n",
    "    \n",
    "    \n",
    "    # --- Animation Part Begins Here ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    x_np = x.numpy()\n",
    "    y_np = y.numpy()\n",
    "    \n",
    "    ax1.plot(x_np, y_np, 'b-', label='sin(x)', linewidth=2)\n",
    "    line_pred, = ax1.plot([], [], 'r-', label='Polynomial', linewidth=2)\n",
    "    ax1.set_xlim(-math.pi, math.pi)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    title = ax1.set_title('')\n",
    "    \n",
    "    loss_line, = ax2.plot([], [], 'g-', linewidth=2)\n",
    "    ax2.set_xlim(0, 2000)\n",
    "    \n",
    "    all_losses = [h['loss'] for h in history]\n",
    "    min_loss = min(all_losses)\n",
    "    max_loss = max(all_losses[:10]) if len(all_losses) > 10 else max(all_losses)\n",
    "    \n",
    "    if min_loss > 0:\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_ylim(min_loss * 0.5, max_loss * 2)\n",
    "    else:\n",
    "        ax2.set_ylim(0, max_loss * 1.1)\n",
    "        print(\"Warning: Loss has non-positive values, using linear scale\")\n",
    "    \n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    iterations_so_far = []\n",
    "    losses_so_far = []\n",
    "    \n",
    "    def init():\n",
    "        line_pred.set_data([], [])\n",
    "        loss_line.set_data([], [])\n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    def animate(frame):\n",
    "        h = history[frame]\n",
    "        \n",
    "        line_pred.set_data(x_np, h['y_pred'].numpy())\n",
    "        \n",
    "        iterations_so_far.append(h['iteration'])\n",
    "        losses_so_far.append(h['loss'])\n",
    "        loss_line.set_data(iterations_so_far, losses_so_far)\n",
    "        \n",
    "        a_val, b_val, c_val, d_val = h['params']\n",
    "        title.set_text(f\"Iteration {h['iteration']}, Loss: {h['loss']:.2f}\\n\"\n",
    "                      f\"y = {a_val:.3f} + {b_val:.3f}x + {c_val:.3f}x² + {d_val:.3f}x³\")\n",
    "        \n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, init_func=init, frames=len(history),\n",
    "                        interval=50, blit=True, repeat=True)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save('polynomial_training.gif', writer='pillow', fps=20)\n",
    "\n",
    "    # --- Animation Part Ends Here ---\n",
    "\n",
    "    # Show Final plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version2-intro"
   },
   "source": [
    "## Version 2: Less manual work (autograd?)\n",
    "\n",
    "- Task 1: Initialize our random weights (Hint: be careful here, maybe take a quick look at the guidebook)\n",
    "- Task 2: Implement the forward Pass (Hint: you did this before...)\n",
    "- Task 3: Loss Calculation (Hint: also something you’ve done before...)\n",
    "- Task 4: Implement backpropagation (Hint: it’s a bit of work? the guidebook will save you)\n",
    "- Task 5: Update the weights (Hint: similar to version 1, but don’t forget the actual objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "version2-run"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Here we create our input and output data\n",
    "    x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "    y = torch.sin(x)\n",
    "\n",
    "    # --- Your implementation starts here ---\n",
    "\n",
    "    # Task 1 --- BEGIN ---\n",
    "    \n",
    "    # Task 1 --- END ---\n",
    "\n",
    "    # Don't Remove ME! (Store history for animation) --- BEGIN ---\n",
    "    history = []\n",
    "    save_every = 10 \n",
    "    # Don't Remove ME! (Store history for animation) --- END ---\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(2000):\n",
    "        # Task 2 --- BEGIN ---\n",
    "        y_prediction = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 2 --- END ---\n",
    "\n",
    "        # Task 3 --- BEGIN ---\n",
    "        loss = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 3 --- END ---\n",
    "        \n",
    "        # Don't Remove ME! (Store history for animation) --- BEGIN ---\n",
    "        if t % save_every == 0:\n",
    "            history.append({\n",
    "                'iteration': t,\n",
    "                'y_pred': y_prediction.detach().clone(),\n",
    "                'loss': loss.item(),\n",
    "                'params': (a.item(), b.item(), c.item(), d.item())\n",
    "            })\n",
    "        \n",
    "        if t % 100 == 99:\n",
    "            print(t, loss.item())\n",
    "        # Don't Remove ME! (Store history for animation) --- END ---\n",
    "\n",
    "        # Task 4 --- BEGIN ---\n",
    "\n",
    "        # Task 4 --- END ---\n",
    "        \n",
    "        # Task 5 --- BEGIN ---\n",
    "\n",
    "        # Task 5 --- END ---\n",
    "        \n",
    "    print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
    "    \n",
    "    # --- Your implementation ends here ---\n",
    "\n",
    "    # --- Animation Part Begins Here ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    x_np = x.numpy()\n",
    "    y_np = y.numpy()\n",
    "    \n",
    "    ax1.plot(x_np, y_np, 'b-', label='sin(x)', linewidth=2)\n",
    "    line_pred, = ax1.plot([], [], 'r-', label='Polynomial', linewidth=2)\n",
    "    ax1.set_xlim(-math.pi, math.pi)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    title = ax1.set_title('')\n",
    "    \n",
    "    loss_line, = ax2.plot([], [], 'g-', linewidth=2)\n",
    "    ax2.set_xlim(0, 2000)\n",
    "    \n",
    "    all_losses = [h['loss'] for h in history]\n",
    "    min_loss = min(all_losses)\n",
    "    max_loss = max(all_losses[:10]) if len(all_losses) > 10 else max(all_losses)\n",
    "    \n",
    "    if min_loss > 0:\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_ylim(min_loss * 0.5, max_loss * 2)\n",
    "    else:\n",
    "        ax2.set_ylim(0, max_loss * 1.1)\n",
    "        print(\"Warning: Loss has non-positive values, using linear scale\")\n",
    "    \n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    iterations_so_far = []\n",
    "    losses_so_far = []\n",
    "\n",
    "\n",
    "\n",
    "    def init():\n",
    "        line_pred.set_data([], [])\n",
    "        loss_line.set_data([], [])\n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    def animate(frame):\n",
    "        h = history[frame]\n",
    "        \n",
    "        line_pred.set_data(x_np, h['y_pred'].numpy())\n",
    "        \n",
    "        iterations_so_far.append(h['iteration'])\n",
    "        losses_so_far.append(h['loss'])\n",
    "        loss_line.set_data(iterations_so_far, losses_so_far)\n",
    "        \n",
    "        a_val, b_val, c_val, d_val = h['params']\n",
    "        title.set_text(f\"Iteration {h['iteration']}, Loss: {h['loss']:.2f}\\n\"\n",
    "                      f\"y = {a_val:.3f} + {b_val:.3f}x + {c_val:.3f}x² + {d_val:.3f}x³\")\n",
    "        \n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, init_func=init, frames=len(history),\n",
    "                        interval=50, blit=True, repeat=True)\n",
    "\n",
    "    # Save animation\n",
    "    anim.save('polynomial_training_ver2.gif', writer='pillow', fps=20)\n",
    "    \n",
    "    # --- Animation Part Ends Here ---\n",
    "    \n",
    "    # Show Final plot        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version3-intro"
   },
   "source": [
    "## Version 3: Let's customize!\n",
    "\n",
    "- Now we’re introducing another approach for finding our parameters: **Legendre Polynomials**\n",
    "- We still want to use autograd, but there’s a tiny issue: PyTorch doesn’t support this out of the box\n",
    "- So our goal is to build a custom autograd function\n",
    "- Our function is $P_{3}(z) = 0.5 \\cdot(5\\cdot z^3 - 3\\cdot z)$ and it's derivative $P'_3(z) = 1.5 \\cdot(5\\cdot z^2 -1)$\n",
    "- Tasks:\n",
    "    - Task 1: Fix the `forwad` function (Hint: we need to save our input)\n",
    "    - Task 2: Using the saved input from the forward pass, fix the `backward` function (Hint: what do we usually compute during the backward pass?)\n",
    "    - Task 3: Implement the forward pass (Hint: the approach changed, which means what we’re predicting also changed)\n",
    "    - Task 4: Implement the loss (yep... here we go again)\n",
    "    - Task 5: Implement backpropagation and update your weights (Hint: very similar to version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "version3-run"
   },
   "outputs": [],
   "source": [
    "# --- Your implementation starts here ---\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function for 3rd degree Legendre polynomial\n",
    "    P3(z) = 0.5 * (5*z^3 - 3*z)\n",
    "    P3'(z) = 1.5 * (5*z^2 - 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        Forward pass: compute P3(input)\n",
    "        Save input for backward pass\n",
    "        \"\"\"\n",
    "        # Task 1 --- BEGIN ---\n",
    "        \n",
    "        # Task 1 --- END ---\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of P3 with respect to input\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "\n",
    "        # Task 2 --- BEGIN ---\n",
    "        grad_input = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 2 --- END ---\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "def version3_custom_autograd():\n",
    "    \"\"\"\n",
    "    Version 3: Custom autograd function with Legendre polynomial\n",
    "    Model: y = a + b * P3(c + d * x)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Version 3\")\n",
    "    \n",
    "    # Here we create our input and output data\n",
    "    x = torch.linspace(-math.pi, math.pi, 2000, device=device)\n",
    "    y_true = torch.sin(x)\n",
    "\n",
    "    # Don't Remove ME! (Parameters already initialized closer to solution for convergence) --- BEGIN ---\n",
    "    a = torch.full((), 0.0, device=device, requires_grad=True)\n",
    "    b = torch.full((), -1.0, device=device, requires_grad=True)\n",
    "    c = torch.full((), 0.0, device=device, requires_grad=True)\n",
    "    d = torch.full((), 0.3, device=device, requires_grad=True)\n",
    "    # Don't Remove ME! (Parameters already initialized closer to solution for convergence) --- END ---\n",
    "    \n",
    "    learning_rate = 5e-6  # We adjust our learning rate\n",
    "    print(f'Initial parameters: a={a.item():.4f}, b={b.item():.4f}, c={c.item():.4f}, d={d.item():.4f}')\n",
    "    \n",
    "    # Don't Remove ME! (alias for the custom function) --- BEGIN ---\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    # Don't Remove ME! (alias for the custom function) --- END ---\n",
    "    \n",
    "    # Don't Remove ME! (History for animation) --- BEGIN ---\n",
    "    history = []\n",
    "    save_every = 10\n",
    "    # Don't Remove ME! (History for animation) --- END ---\n",
    "    \n",
    "    losses = []\n",
    "    for t in range(2000):\n",
    "        \n",
    "        # Task 3 --- BEGIN ---\n",
    "        y_pred = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 3 --- END ---\n",
    "\n",
    "        # Task 4 --- BEGIN ---\n",
    "        loss = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 4 --- END ---\n",
    "\n",
    "        # Don't Remove ME! (Storing losses for animation) --- BEGIN ---\n",
    "        losses.append(loss.item())\n",
    "        # Don't Remove ME! (Storing losses for animation) --- END ---\n",
    "\n",
    "\n",
    "        # Don't Remove ME! (Store history for animation) --- BEGIN ---\n",
    "        if t % save_every == 0:\n",
    "            history.append({\n",
    "                'iteration': t,\n",
    "                'y_pred': y_pred.detach().clone(),\n",
    "                'loss': loss.item(),\n",
    "                'params': (a.item(), b.item(), c.item(), d.item())\n",
    "            })\n",
    "        \n",
    "        if t % 100 == 99:\n",
    "            print(f'Epoch {t+1:4d}, Loss: {loss.item():.6f}')\n",
    "        # Don't Remove ME! (Store history for animation) --- END ---\n",
    "        \n",
    "        # Task 5 --- BEGIN ---\n",
    "\n",
    "        # Task 5 --- END ---\n",
    "    # --- Your implementation ends here ---\n",
    "    \n",
    "    print(f'\\nFinal parameters:')\n",
    "    print(f'  a = {a.item():.6f}')\n",
    "    print(f'  b = {b.item():.6f}')\n",
    "    print(f'  c = {c.item():.6f}')\n",
    "    print(f'  d = {d.item():.6f}')\n",
    "    print(f'Final model: y = {a.item():.6f} + {b.item():.6f} * P3({c.item():.6f} + {d.item():.6f} * x)')\n",
    "    print(f'Final Loss: {losses[-1]:.6f}')\n",
    "    \n",
    "    # --- Animation Part Begins Here ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y_true.cpu().numpy()\n",
    "    \n",
    "    # Initialize plots\n",
    "    ax1.plot(x_np, y_np, 'b-', label='sin(x)', linewidth=2)\n",
    "    line_pred, = ax1.plot([], [], 'r-', label='Legendre Polynomial Model', linewidth=2)\n",
    "    ax1.set_xlim(-math.pi, math.pi)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    title = ax1.set_title('')\n",
    "    \n",
    "    # Loss plot\n",
    "    loss_line, = ax2.plot([], [], 'g-', linewidth=2)\n",
    "    ax2.set_xlim(0, 2000)\n",
    "    \n",
    "    # Get min/max loss for proper y-axis scaling\n",
    "    all_losses = [h['loss'] for h in history]\n",
    "    min_loss = min(all_losses)\n",
    "    max_loss = max(all_losses[:10]) if len(all_losses) > 10 else max(all_losses)\n",
    "    \n",
    "    # Only use log scale if all losses are positive\n",
    "    if min_loss > 0:\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_ylim(min_loss * 0.5, max_loss * 2)\n",
    "    else:\n",
    "        ax2.set_ylim(0, max_loss * 1.1)\n",
    "        print(\"Warning: Loss has non-positive values, using linear scale\")\n",
    "    \n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    iterations_so_far = []\n",
    "    losses_so_far = []\n",
    "    \n",
    "    def init():\n",
    "        line_pred.set_data([], [])\n",
    "        loss_line.set_data([], [])\n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    def animate(frame):\n",
    "        h = history[frame]\n",
    "        \n",
    "        line_pred.set_data(x_np, h['y_pred'].cpu().numpy())\n",
    "        \n",
    "        iterations_so_far.append(h['iteration'])\n",
    "        losses_so_far.append(h['loss'])\n",
    "        loss_line.set_data(iterations_so_far, losses_so_far)\n",
    "        \n",
    "        a_val, b_val, c_val, d_val = h['params']\n",
    "        title.set_text(f\"Iteration {h['iteration']}, Loss: {h['loss']:.2f}\\n\"\n",
    "                      f\"y = {a_val:.3f} + {b_val:.3f} × P₃({c_val:.3f} + {d_val:.3f}x)\")\n",
    "        \n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, init_func=init, frames=len(history),\n",
    "                        interval=50, blit=True, repeat=True)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save('polynomial_training_ver3_legendre.gif', writer='pillow', fps=20)\n",
    "    \n",
    "    # --- Animation Part Ends Here ---\n",
    "    \n",
    "    \n",
    "    # Show Final plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return a, b, c, d, x, y_true, losses\n",
    "\n",
    "a3, b3, c3, d3, x_data3, y_true3, losses3 = version3_custom_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version4-intro"
   },
   "source": [
    "## Version 4: Let's optimize!\n",
    "\n",
    "\n",
    "- Now that we’re used to customizing things, let’s go back to our old function.\n",
    "- This time, we’ll build everything using `torch.nn.Module` (this makes the whole workflow systematic and modular).\n",
    "- Tasks: you’re not allowed to do anything manually anymore!\n",
    "    - Task 1: Define your parameters.\n",
    "    - Task 2: Define your model (hint: we’re making a class for this, remember?).\n",
    "    - Task 3: Define the loss you’ll use (hint: it’s MSE Loss, and no, we’re not doing the math ourselves).\n",
    "    - Task 4: Define your optimizer (advice: check the guidebook).\n",
    "    - Task 5: Implement your forward pass (hint: you already defined your model, use it).\n",
    "    - Task 6: Compute your loss (hint: you set this up in Task 3).\n",
    "    - Task 7: Clear your gradients, compute them, then update your weights (hint: less than 4 lines of code, everything you need has already been defined).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "version4-run"
   },
   "outputs": [],
   "source": [
    "# --- Your implementation starts here ---\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        # Task 1 --- BEGIN ---\n",
    "\n",
    "        # Task 1 --- END ---\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Get current parameter values as dictionary\"\"\"\n",
    "        return {\n",
    "            'a': self.a.item(),\n",
    "            'b': self.b.item(),\n",
    "            'c': self.c.item(),\n",
    "            'd': self.d.item()\n",
    "        }\n",
    "    \n",
    "    def string(self):\n",
    "        \"\"\"String representation of the polynomial\"\"\"\n",
    "        params = self.get_params()\n",
    "        return f'y = {params[\"a\"]:.6f} + {params[\"b\"]:.6f}x + {params[\"c\"]:.6f}x² + {params[\"d\"]:.6f}x³'\n",
    "\n",
    "def version4_nn_module():\n",
    "    \"\"\"\n",
    "    Version 4: Using nn.Module and built-in optimizer\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Version 4 ===\")\n",
    "    \n",
    "    # Create input and target data\n",
    "    x = torch.linspace(-math.pi, math.pi, 2000, device=device)\n",
    "    y_true = torch.sin(x)\n",
    "\n",
    "    # Task 2 --- BEGIN ---\n",
    "    model = \"NOT YET IMPLEMENTED\"\n",
    "    # Task 2 --- END ---\n",
    "        \n",
    "    # Task 3 --- BEGIN ---\n",
    "    criterion = \"NOT YET IMPLEMENTED\"\n",
    "    # Task 3 --- END ---\n",
    "\n",
    "    # Task 4 --- BEGIN ---\n",
    "    optimizer = \"NOT YET IMPLEMENTED\"\n",
    "    # Task 4 --- END ---\n",
    "\n",
    "    print(f'Initial parameters: {model.get_params()}')\n",
    "    \n",
    "    # Don't Remove ME! (History for animation) --- BEGIN ---\n",
    "    history = []\n",
    "    save_every = 10\n",
    "    \n",
    "    losses = []\n",
    "    # Don't Remove ME! (History for animation) --- END ---\n",
    "    \n",
    "    for t in range(2000):\n",
    "\n",
    "        # Task 5 --- BEGIN ---\n",
    "        y_pred = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 5 --- END ---        \n",
    "        \n",
    "        # Task 6 --- BEGIN ---\n",
    "        loss = \"NOT YET IMPLEMENTED\"\n",
    "        # Task 6 ---- END ---\n",
    "        \n",
    "        \n",
    "        # Don't Remove ME! (Store history for animation) --- BEGIN ---\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if t % save_every == 0:\n",
    "            history.append({\n",
    "                'iteration': t,\n",
    "                'y_pred': y_pred.detach().clone(),\n",
    "                'loss': loss.item(),\n",
    "                'params': model.get_params()\n",
    "            })\n",
    "        \n",
    "        if t % 100 == 99:\n",
    "            print(f'Epoch {t+1:4d}, Loss: {loss.item():.6f}')\n",
    "        # Don't Remove ME! (Store history for animation) --- END ---\n",
    "\n",
    "        # Task 7 --- BEGIN ---\n",
    "\n",
    "        # Task 7 --- END ---\n",
    "    \n",
    "    # --- Your implementation ends here ---\n",
    "    final_params = model.get_params()\n",
    "\n",
    "    # Don't Remove ME! (Final parameter and loss values) --- BEGIN ---\n",
    "    print(f'\\nFinal parameters:')\n",
    "    print(f'  a = {final_params[\"a\"]:.6f}')\n",
    "    print(f'  b = {final_params[\"b\"]:.6f}')\n",
    "    print(f'  c = {final_params[\"c\"]:.6f}')\n",
    "    print(f'  d = {final_params[\"d\"]:.6f}')\n",
    "    print(f'Final model: {model.string()}')\n",
    "    print(f'Final Loss: {losses[-1]:.6f}')\n",
    "    # Don't Remove ME! (Final parameter and loss values) --- END ---\n",
    "    \n",
    "    # --- Animation Part Begins Here ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y_true.cpu().numpy()\n",
    "    \n",
    "    ax1.plot(x_np, y_np, 'b-', label='sin(x)', linewidth=2)\n",
    "    line_pred, = ax1.plot([], [], 'r-', label='nn.Module Polynomial', linewidth=2)\n",
    "    ax1.set_xlim(-math.pi, math.pi)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    title = ax1.set_title('')\n",
    "    \n",
    "    loss_line, = ax2.plot([], [], 'g-', linewidth=2)\n",
    "    ax2.set_xlim(0, 2000)\n",
    "    \n",
    "    all_losses = [h['loss'] for h in history]\n",
    "    min_loss = min(all_losses)\n",
    "    max_loss = max(all_losses[:10]) if len(all_losses) > 10 else max(all_losses)\n",
    "    \n",
    "    if min_loss > 0:\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_ylim(min_loss * 0.5, max_loss * 2)\n",
    "    else:\n",
    "        ax2.set_ylim(0, max_loss * 1.1)\n",
    "        print(\"Warning: Loss has non-positive values, using linear scale\")\n",
    "    \n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    iterations_so_far = []\n",
    "    losses_so_far = []\n",
    "    \n",
    "    def init():\n",
    "        line_pred.set_data([], [])\n",
    "        loss_line.set_data([], [])\n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    def animate(frame):\n",
    "        h = history[frame]\n",
    "        \n",
    "        line_pred.set_data(x_np, h['y_pred'].cpu().numpy())\n",
    "        \n",
    "        iterations_so_far.append(h['iteration'])\n",
    "        losses_so_far.append(h['loss'])\n",
    "        loss_line.set_data(iterations_so_far, losses_so_far)\n",
    "        \n",
    "        params = h['params']\n",
    "        title.set_text(f\"Iteration {h['iteration']}, Loss: {h['loss']:.2f}\\n\"\n",
    "                      f\"y = {params['a']:.3f} + {params['b']:.3f}x + {params['c']:.3f}x² + {params['d']:.3f}x³\")\n",
    "        \n",
    "        return line_pred, loss_line, title\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, init_func=init, frames=len(history),\n",
    "                        interval=50, blit=True, repeat=True)\n",
    "    \n",
    "    anim.save('polynomial_training_ver4_nn_module.gif', writer='pillow', fps=20)\n",
    "    # --- Animation Part Ends Here ---\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, x, y_true, losses\n",
    "\n",
    "# Run Version 4\n",
    "model4, x_data4, y_true4, losses4 = version4_nn_module()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "teach (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
