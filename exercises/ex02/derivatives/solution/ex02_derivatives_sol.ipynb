{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffd9458",
   "metadata": {},
   "source": [
    "# Derivatives and Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468aa73a",
   "metadata": {},
   "source": [
    "## Run The Next Cell \n",
    "Like the tensors exercise, we are going to use torch! We also need another library for fun visuals: Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d94abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d45ae3",
   "metadata": {},
   "source": [
    "### Note: In case you are not familiar with writing math formulas using Markdown, it's time to learn!\n",
    "Here is a nice cheat sheet for that: https://www.upyesp.org/posts/makrdown-vscode-math-notation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3aeafe",
   "metadata": {},
   "source": [
    "## Derivatives Refresher\n",
    "Given a function $f(x)$, the derivative $f'(x)$ measures how much $f(x)$ changes when $x$ changes.\n",
    "\n",
    "**Exercise 1:** Derive the derivative of the following function:\n",
    "$$\n",
    "f(x) = 3x^3 - 2x^2 + 5x - 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb8a8a",
   "metadata": {},
   "source": [
    "**Your Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1187e",
   "metadata": {},
   "source": [
    "$$f'(x)=9x^2-4x+5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bf03e",
   "metadata": {},
   "source": [
    "**Exercise 2:** Write $f(x)$ and $f'(x)$ as two functions and try to visualize them.\n",
    "\n",
    "**Your Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee751dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**3 - 2*x**2 + 5*x - 4\n",
    "\n",
    "def f_prime(x):\n",
    "    return 9*x**2 - 4*x + 5\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "plt.plot(x, f(x), label='f(x)')\n",
    "plt.plot(x, f_prime(x), label=\"f'(x)\")\n",
    "plt.legend()\n",
    "plt.title('Random Function and its Derivative')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5462d3",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "The sigmoid function is defined as:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Exercise 1:** Calculate its derivative and express it in terms of $\\sigma(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd412728",
   "metadata": {},
   "source": [
    "**Your Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44a1c6",
   "metadata": {},
   "source": [
    "\n",
    "Given the sigmoid:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}\n",
    "$$\n",
    "\n",
    "we compute its derivative $\\sigma'(x)$.\n",
    "\n",
    "$$\n",
    "\\sigma(x) = (1 + e^{-x})^{-1}\n",
    "$$\n",
    "\n",
    "Let $g(u) = u^{-1}$ and $u(x) = 1 + e^{-x}$ (We are applying the chain rule)\n",
    "\n",
    "$$\n",
    "\\sigma(x) = g(u(x))\n",
    "$$\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\sigma(x) = g'(u(x)) \\cdot u'(x)\n",
    "\n",
    "$$\n",
    "\n",
    "We have $g'(u) = -u^{-2}$ and $u'(x) = \\frac{d}{dx}(1 + e^{-x}) = -e^{-x}$.  \n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = - (1 + e^{-x})^{-2} \\cdot (-e^{-x}) = \\frac{e^{-x}}{(1 + e^{-x})^{2}}\n",
    "$$\n",
    "\n",
    "Write the result as a product:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Recognize the first factor as $\\sigma(x)$.  \n",
    "\n",
    "For the second factor, observe:\n",
    "\n",
    "$$\n",
    "\\frac{e^{-x}}{1 + e^{-x}} = \\frac{1 + e^{-x} - 1}{1 + e^{-x}} = 1 - \\frac{1}{1 + e^{-x}} = 1 - \\sigma(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\sigma'(x) = \\sigma(x)\\,\\bigl(1 - \\sigma(x)\\bigr)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4b359",
   "metadata": {},
   "source": [
    "**Exercise 1:** Implement the sigmoid function and its derivative, then visualize both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "x = torch.linspace(-8, 8, 200)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid(x)')\n",
    "plt.plot(x, sigmoid_derivative(x), label=\"sigmoid'(x)\")\n",
    "plt.title('Sigmoid and its Derivative')\n",
    "plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaefbb9",
   "metadata": {},
   "source": [
    "## 4. ReLU Function\n",
    "The Rectified Linear Unit (ReLU) is defined as:\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Exercise 1:** Calculate the derivative of ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4ee35",
   "metadata": {},
   "source": [
    "**Your Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bf849",
   "metadata": {},
   "source": [
    "\n",
    "Given the Rectified Linear Unit (ReLU) function:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "we compute its derivative $\\text{ReLU}'(x)$.\n",
    "\n",
    "\n",
    "\n",
    "The ReLU function can be written as a piecewise function:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{if } x > 0, \\\\\n",
    "0, & \\text{if } x \\leq 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For $x > 0$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\text{ReLU}(x) = \\frac{d}{dx}(x) = 1\n",
    "$$\n",
    "\n",
    "For $x < 0$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\text{ReLU}(x) = \\frac{d}{dx}(0) = 0\n",
    "$$\n",
    "\n",
    "At $x = 0$, the derivative is undefined because of the discontinuity in slope (the function is continuous but not differentiable at 0).  \n",
    "In practice, deep learning frameworks define it as either 0 or 1 at $x = 0$ for convenience, most commonly **0**.\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0, \\\\\n",
    "0 & \\text{if } x \\leq 0.\n",
    "\\end{cases}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a64ff",
   "metadata": {},
   "source": [
    "**Exercise 2:** Implement ReLU and its derivative, then visualize both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a181b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(torch.tensor(0.0), x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).float()\n",
    "\n",
    "x = torch.linspace(-5, 5, 200)\n",
    "plt.plot(x, relu(x), label='ReLU(x)')\n",
    "plt.plot(x, relu_derivative(x), label=\"ReLU'(x)\")\n",
    "plt.title('ReLU and its Derivative')\n",
    "plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2e4ad",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy Loss\n",
    "For a single prediction $y_{pred}$ and true label $y \\in \\{0, 1\\}$, the **binary cross-entropy loss** is:\n",
    "$$\n",
    "\\mathcal{L} = -[y \\log(y_{pred}) + (1 - y) \\log(1 - y_{pred})]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71dd334",
   "metadata": {},
   "source": [
    "**Exercise 1: calculate the derivative with respect to $y_{pred}$**\n",
    "\n",
    "**Your Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641aea4",
   "metadata": {},
   "source": [
    "We derive the gradient of the **binary cross-entropy loss** with respect to the model output (logit).  \n",
    "This is the most fundamental form used in binary classification neural networks.\n",
    "\n",
    "\n",
    "Let the model output (before activation) be $z$, and the **sigmoid function** be\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The predicted probability is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z)\n",
    "$$\n",
    "\n",
    "and the true label is $y \\in \\{0, 1\\}$.\n",
    "\n",
    "The **binary cross-entropy loss** is defined as:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\big( y \\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}) \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "Differentiate $L$ with respect to $\\hat{y}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\n",
    "= - \\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\n",
    "= \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}\n",
    "$$\n",
    "\n",
    "Since $\\hat{y} = \\sigma(z)$, and\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)\\,(1 - \\sigma(z)) = \\hat{y}(1 - \\hat{y}),\n",
    "$$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "We want $\\Large{\\frac{\\partial L}{\\partial z}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\n",
    "= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}\n",
    "$$\n",
    "\n",
    "Substitute both expressions:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z}\n",
    "= \\left( \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})} \\right) \\cdot \\hat{y}(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "The derivative of the **binary cross-entropy loss** with respect to the model logit $z$ is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial z} = \\sigma(z) - y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317ce8f",
   "metadata": {},
   "source": [
    "**Exercise 2:** Implement *BCE* and its derivative, then visualize both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c230ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y):\n",
    "    eps = 1e-8\n",
    "    return - (y * torch.log(y_pred + eps) + (1 - y) * torch.log(1 - y_pred + eps))\n",
    "\n",
    "def bce_derivative(y_pred, y):\n",
    "    eps = 1e-8\n",
    "    return - (y / (y_pred + eps)) + ((1 - y) / (1 - y_pred + eps))\n",
    "\n",
    "y_pred = torch.linspace(0.01, 0.99, 100)\n",
    "\n",
    "for label in [0, 1]:\n",
    "    plt.plot(y_pred, binary_cross_entropy(y_pred, torch.tensor(label)), label=f'y={label}')\n",
    "plt.title('Binary Cross-Entropy Loss')\n",
    "plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "for label in [0, 1]:\n",
    "    plt.plot(y_pred, bce_derivative(y_pred, torch.tensor(label)), label=f\"dL/dy_pred (y={label})\")\n",
    "plt.title('Derivative of BCE Loss')\n",
    "plt.legend(); plt.grid(True); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
