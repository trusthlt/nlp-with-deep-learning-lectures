{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hdr1",
   "metadata": {},
   "source": [
    "# Is our Puppet capable?\n",
    "- In the *binary classification* fine-tuning guided exercise, we worked with a relatively simple dataset (fake vs. true news). Because of the structure and cleanliness of that dataset, our fine-tuned model performed extremely well. That’s not always the case, though many real-world datasets are noisy, inconsistently formatted, or contain labels that are highly subjective. To succeed in those situations, you need more extensive pre-processing, careful data handling, and often quite a bit of trial and error in your training setup\n",
    "- For this next task, we will be working with the [LIAR-PLUS](https://github.com/Tariq60/LIAR-PLUS) dataset, which is dataset introduced in the [Liar, Liar Pants on Fire](https://aclanthology.org/P17-2067/) paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a3f23",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr2",
   "metadata": {},
   "source": [
    "#### Loading our data and cleaning it\n",
    "- The [LIAR-PLUS](https://github.com/Tariq60/LIAR-PLUS) dataset is formed of 3 tsv files (`train.csv`, `val.csv` and `test.csv`)\n",
    "- This is the description of the entries according to the creator:\n",
    "    - Column 1: the ID of the statement ([ID].json).\n",
    "    - Column 2: the **label**.\n",
    "    - Column 3: the **statement**.\n",
    "    - Column 4: the subject(s).\n",
    "    - Column 5: the speaker.\n",
    "    - Column 6: the speaker's job title.\n",
    "    - Column 7: the state info.\n",
    "    - Column 8: the party affiliation.\n",
    "    - Columns 9-13: the total credit history count, including the current statement.\n",
    "        - 9: barely true counts.\n",
    "        - 10: false counts.\n",
    "        - 11: half true counts.\n",
    "        - 12: mostly true counts.\n",
    "        - 13: pants on fire counts.\n",
    "    - Column 14: the context (venue / location of the speech or statement).\n",
    "    - Column 15: the extracted justification\n",
    "##### What we actually need:\n",
    "- For the fine-tuning of our model we will only be needing the `statement` and the `label`, so let's do some quick data cleaning\n",
    "- Tasks:\n",
    "    - Task 1: we want to finish the implementation of the `load_label_text_tsv` function, fields in tsv are tab seperated, you have the description of the dataset, do your work\n",
    "    - Task 2: you will end up with few data points that have the label as a `'nan'` string (not `None`), remove those, we won't need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_text_tsv(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str)\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"File {path} has fewer than 4 columns.\")\n",
    "    out = pd.DataFrame({\n",
    "        # --- Task 1 begins here ---\n",
    "        \"label\": \"NOT IMPLEMENTED YET\",\n",
    "        \"text\":  \"NOT IMPLEMENTED YET\"\n",
    "        # --- Task 1 ends here ---\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "train_df = load_label_text_tsv('conspiracy/train.tsv')\n",
    "val_df   = load_label_text_tsv('conspiracy/val.tsv')\n",
    "test_df  = load_label_text_tsv('conspiracy/test.tsv')\n",
    "\n",
    "# drop labels with 'nan' label\n",
    "# --- Task 2 begins here ---\n",
    "train_df = \"NOT IMPLEMENTED YET\"\n",
    "val_df   = \"NOT IMPLEMENTED YET\"\n",
    "test_df  = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr3",
   "metadata": {},
   "source": [
    "#### Textual labels are for humans, not for our Puppet\n",
    "- Let's say you have 3 labels: \"Salty\", \"Sweet\", \"Bland\", it's better to call them 0 (for \"Salty\"), 1 (for \"Sweet\"), 2 (for \"Bland\")\n",
    "- Tasks:\n",
    "    - Task 1: get our labels\n",
    "    - Task 2: build 2 dictionaries `label2id` (label is key, id is value), `id2label` (id is key, label is value)\n",
    "    - Task 3: create a new field for your dataframes for our ids (Hint: what did you create in Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1 begins here ---\n",
    "unique_labels = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 1 ends here ---\n",
    "\n",
    "# --- Task 2 begins here ---\n",
    "label2id = \"NOT IMPLEMENTED YET\"\n",
    "id2label = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "print('Label → ID mapping:', label2id)\n",
    "\n",
    "# --- Task 3 begins here ---\n",
    "train_df['label_id'] = \"NOT IMPLEMENTED YET\"\n",
    "val_df['label_id']   = \"NOT IMPLEMENTED YET\"\n",
    "test_df['label_id']  = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 3 ends here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr4",
   "metadata": {},
   "source": [
    "#### To HuggingFace dataset\n",
    "- We want to transform our dataframe to a HuggingFace dataset\n",
    "- Tasks:\n",
    "    - Task 1: finish the implementation of `hf_format`, this method takes a dataframe, only it should only keep `text` and `label_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hfdata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_format(df):\n",
    "    # --- Task 1 begins here ---\n",
    "    return \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 1 ends here ---\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': hf_format(train_df),\n",
    "    'validation': hf_format(val_df),\n",
    "    'test': hf_format(test_df)\n",
    "})\n",
    "\n",
    "print(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr5",
   "metadata": {},
   "source": [
    "#### Tokenize the dataset\n",
    "- We will be using `bert-base-uncased` as our model\n",
    "- Tasks:\n",
    "    - Task 1: initialize our `tokenizer`\n",
    "    - Task 2: create our tokenized dataset\n",
    "    - Task 3: set the dataset format to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1 begins here ---\n",
    "tokenizer = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 1 ends here ---\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# --- Task 2 begins here ---\n",
    "dataset = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "dataset = dataset.remove_columns(['text'])\n",
    "dataset = dataset.rename_column('label_id','labels')\n",
    "\n",
    "# --- Task 3 begins here ---\n",
    "\n",
    "\n",
    "# --- Task 3 ends here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr6",
   "metadata": {},
   "source": [
    "#### Creating a baseline using our non fine-tuned model\n",
    "- The model is initialized\n",
    "- Tasks:\n",
    "    - Task 1: set the baseline model to evaluating mode\n",
    "    - Task 2: get the model prediction (Hint: this is a multi class problem, what do we need?)\n",
    "    - Task 3: get the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "baseline_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=8)\n",
    "logits_all = []\n",
    "\n",
    "# --- Task 1 begins here ---\n",
    "\n",
    "# --- Task 1 ends here ---\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Baseline Evaluation\"):\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        outputs = baseline_model(**batch)\n",
    "        logits_all.append(outputs.logits.cpu())\n",
    "\n",
    "logits = torch.cat(logits_all, dim=0)\n",
    "\n",
    "# --- Task 2 begins here ---\n",
    "baseline_preds = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 2 ends here ---\n",
    "\n",
    "# --- Task 3 begins here ---\n",
    "true_labels = \"NOT IMPLEMENTED YET\"\n",
    "# --- Task 3 ends here ---\n",
    "\n",
    "print(\"\\nBaseline (Before Fine-Tuning)\")\n",
    "\n",
    "\n",
    "print(classification_report(true_labels, baseline_preds, target_names=unique_labels))\n",
    "\n",
    "cm = confusion_matrix(true_labels, baseline_preds)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.title('Confusion Matrix (Baseline)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr7",
   "metadata": {},
   "source": [
    "#### We load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loadmodel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr8",
   "metadata": {},
   "source": [
    "#### Training Setup\n",
    "- Usually when training/fine-tuning a model, we observe the dev (some call the val set) and we try to optimize our hyperparameters to enhance the model performance (while avoiding overfitting)\n",
    "- Tasks:\n",
    "    - Task 1: initialize the `trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LR = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_6class_output',\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # --- Task 1 begins here ---\n",
    "    model=\"NOT IMPLEMENTED YET\",\n",
    "    args=\"NOT IMPLEMENTED YET\",\n",
    "    train_dataset=\"NOT IMPLEMENTED YET\",\n",
    "    eval_dataset=\"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 1 ends here --\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr9",
   "metadata": {},
   "source": [
    "#### Let the training begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdr10",
   "metadata": {},
   "source": [
    "#### Let's see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_after",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = trainer.predict(dataset['test'])\n",
    "pred_labels = np.argmax(pred_output.predictions, axis=1)\n",
    "true_labels = dataset['test']['labels']\n",
    "\n",
    "print(\"\\nAfter Fine-Tuning\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=unique_labels))\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (After Fine-Tuning)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16247b14",
   "metadata": {},
   "source": [
    "#### What????\n",
    "- If you see low values compared to the guided exercise, don't be disappointed, the writers of the paper got even worse results!\n",
    "- An encoder may not enough to solve such a task (a task that require prior knowledge, reasoning capabilities, etc...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
