{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf19277b",
   "metadata": {},
   "source": [
    "# Word 2 what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d53fe",
   "metadata": {},
   "source": [
    "- As usual, we need to import some libraries.\n",
    "- Make sure you have installed the spaCy English package (if you haven't: `python -m spacy download en_core_web_sm`).\n",
    "\n",
    "- Tasks:\n",
    "    - **Task 1**: Implement the `tokenize` function so it returns a list of tokens when given a text.\n",
    "    - **Task 2**: Now that we have our tokens, let's build our vocabulary! The `build_vocab` function you are implementing should return:\n",
    "        - A **list** containing our vocabulary\n",
    "        - A **dictionary** linking each unique token with an index (`{'hi': 0, 'name': 1, 'my': 2, ...}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038de264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.animation as animation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# make sure to download the spacy model with: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# our example text we will be using for this exercise \n",
    "# this was generated using deepseek, you can change it if you want, this example was used cause our models are a bit dumb\n",
    "with open('example_text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "def tokenize(text):\n",
    "    # --- Task 1 begins here ---\n",
    "    doc = \"NOT IMPLEMENTED YET\"\n",
    "    return \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 1 ends here ---\n",
    "\n",
    "print(\"Tokenized Text: \", tokenize(text))\n",
    "print(\"We got\",len(set(tokenize(text))), \"unique tokens\")\n",
    "\n",
    "def build_vocab(tokens, min_count=1):\n",
    "    # --- Task 2 begins here ---\n",
    "    counts = \"NOT IMPLEMENTED YET\"\n",
    "    vocab = \"NOT IMPLEMENTED YET\"\n",
    "    idx = \"NOT IMPLEMENTED YET\"\n",
    "    return vocab, idx\n",
    "    # --- Task 2 ends here ---\n",
    "    \n",
    "print(\"Building vocab...\", build_vocab(tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b940b",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "- Execute this code, we will need these functions later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "# Neareast neighbors function: computes the k nearest neighbors of a given word in the embedding space\n",
    "def nearest_neighbors(word, vocab, idx, embeddings, k=5):\n",
    "    if word not in idx:\n",
    "        return []\n",
    "    i = idx[word]\n",
    "    v = embeddings[i]\n",
    "    norms = np.linalg.norm(embeddings, axis=1)\n",
    "    sims = embeddings.dot(v) / (norms * np.linalg.norm(v) + 1e-9)\n",
    "    nearest = np.argsort(-sims)\n",
    "    return [(vocab[n], float(sims[n])) for n in nearest[1:k+1]]\n",
    "\n",
    "# We use this function to create an animation of the embeddings over time\n",
    "def animate_embeddings_gif(snapshots, vocab, save_path='skip_grams.gif', n_words=19, fps=12, figsize=(8,8)):\n",
    "\n",
    "    n_words = min(n_words, len(vocab))\n",
    "    vs = snapshots[0].shape[0]\n",
    "\n",
    "    stacked = np.vstack(snapshots)\n",
    "    pca = PCA(n_components=2)\n",
    "    stacked_2d = pca.fit_transform(stacked)\n",
    "\n",
    "    n_frames = len(snapshots)\n",
    "    frames_coords = [ stacked_2d[i*vs:(i+1)*vs, :2][:n_words] for i in range(n_frames) ]\n",
    "    words = vocab[:n_words]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    scatter = ax.scatter(frames_coords[0][:,0], frames_coords[0][:,1], s=40)\n",
    "    texts = [ax.text(x, y, w, fontsize=12) for (x,y), w in zip(frames_coords[0], words)]\n",
    "\n",
    "    all_xy = np.vstack(frames_coords)\n",
    "    margin = 0.5\n",
    "    ax.set_xlim(all_xy[:,0].min()-margin, all_xy[:,0].max()+margin)\n",
    "    ax.set_ylim(all_xy[:,1].min()-margin, all_xy[:,1].max()+margin)\n",
    "\n",
    "    def update(i):\n",
    "        coords = frames_coords[i]\n",
    "        scatter.set_offsets(coords)\n",
    "        for t, (x,y) in zip(texts, coords):\n",
    "            t.set_position((x,y))\n",
    "        ax.set_title(f'Embeddings — frame {i+1}/{n_frames}')\n",
    "        return [scatter, *texts]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=n_frames, interval=1000//fps, blit=False)\n",
    "\n",
    "    writer = animation.PillowWriter(fps=fps)\n",
    "    ani.save(save_path, writer=writer)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved GIF to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f158d87",
   "metadata": {},
   "source": [
    "# Continous Bag of Words\n",
    "- Now that we have our tokens and vocab, we will start creating our simple word2vec model (we will be using the **Skip-gram** architecture for this part)\n",
    "- Tasks:\n",
    "    - Task 1: produce examples of (context_indices, center_index). Implement `generated_cbow_examples`\n",
    "    - Task 2: create an embedding matrix and an output layer that maps embedding_dim → vocab_size. Initialize weights small and random\n",
    "    - Task 3: instantiate model, an optimizer (e.g. SGD), and a loss (CrossEntropyLoss)\n",
    "    - Task 4: forward pass\n",
    "    - Task 5: compute loss, backpropagate, step optimizer\n",
    "    - Task 6: return input embeddings and output weights (Hint: transpose when necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_examples(tokens, idx, window=2):\n",
    "    examples = []\n",
    "    # --- Task 1 begins here ---\n",
    "\n",
    "    # --- Task 1 ends here ---\n",
    "    return examples\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "        # --- Task 2 begins here ---\n",
    "        with torch.no_grad():\n",
    "            self.embeddings.weight.data = \"NOT IMPLEMENTED YET\"\n",
    "            self.output_layer.weight.data = \"NOT IMPLEMENTED YET\"\n",
    "        # --- Task 2 ends here ---\n",
    "        \n",
    "    def forward(self, context_indices):\n",
    "        context_embeds = self.embeddings(context_indices)\n",
    "        embed = context_embeds.mean(dim=0)\n",
    "        scores = self.output_layer(embed)\n",
    "        return scores\n",
    "    \n",
    "\n",
    "\n",
    "    def train_cbow(examples, vocab_size, embedding_dim=16, lr=0.05, epochs=200, print_every=50, record_every=1):\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # --- Task 3 begins here ---\n",
    "        model = \"NOT IMPLEMENTED YET\"\n",
    "        optimizer = \"NOT IMPLEMENTED YET\"\n",
    "        criterion = \"NOT IMPLEMENTED YET\"\n",
    "        # --- Task 3 ends here ---\n",
    "        \n",
    "        snapshots = []\n",
    "        \n",
    "        for ep in range(1, epochs + 1):\n",
    "\n",
    "            indices = torch.randperm(len(examples))\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for idx in indices:\n",
    "                context_idxs, center = examples[idx]\n",
    "                \n",
    "                context_tensor = torch.tensor(context_idxs, dtype=torch.long, device=device)\n",
    "                center_tensor = torch.tensor([center], dtype=torch.long, device=device)\n",
    "                \n",
    "                # --- Task 4 begins here ---\n",
    "                \"\"\"Something missing here?\"\"\"\n",
    "                scores = \"NOT IMPLEMENTED YET\"\n",
    "                loss = \"NOT IMPLEMENTED YET\"\n",
    "                # --- Task 4 ends here ---\n",
    "                \n",
    "                # --- Task 5 begins here ---\n",
    "                \n",
    "                # --- Task 5 ends here ---\n",
    "            \n",
    "            # --- For Animation Purposes ---\n",
    "            if record_every and (ep % record_every == 0 or ep == epochs):\n",
    "                snapshots.append(model.embeddings.weight.data.cpu().numpy().copy())\n",
    "            # --- For Animation Purposes ---\n",
    "            \n",
    "            if ep % print_every == 0 or ep == 1:\n",
    "                avg_loss = total_loss / len(examples)\n",
    "                print(f\"CBOW Epoch {ep}/{epochs}  avg_loss={avg_loss:.4f}\")\n",
    "\n",
    "        # --- Task 6 begins here ---\n",
    "        W_in = \"NOT IMPLEMENTED YET\"\n",
    "        W_out = \"NOT IMPLEMENTED YET\"\n",
    "        # --- Task 6 ends here ---\n",
    "        \n",
    "        return W_in, W_out, snapshots\n",
    "\n",
    "# we use our tokenization function\n",
    "tokens = tokenize(text)\n",
    "# we build our vocabulary and index mapping\n",
    "vocab, idx = build_vocab(tokens)\n",
    "\n",
    "# we use our CBOW data generation function\n",
    "cbow_examples = generate_cbow_examples(tokens, idx, window=2)\n",
    "\n",
    "# we train our CBOW model\n",
    "W_in_cb, W_out_cb, snapshots_cb = CBOW.train_cbow(\n",
    "    cbow_examples, vocab_size=len(vocab), embedding_dim=16, lr=0.01, epochs=400, print_every=100, record_every=1\n",
    ")\n",
    "\n",
    "# we use our helper function to see nearest neighbors of some words in the learned embedding space\n",
    "for w in ['cat', 'dog', 'mat', 'sat', 'rug']:\n",
    "    if w in idx:\n",
    "        print('Nearest to', w, ':', nearest_neighbors(w, vocab, idx, W_in_cb, k=5))\n",
    "    else:\n",
    "        print('\\nWord', w, 'not in vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e1268",
   "metadata": {},
   "source": [
    "# The Fun Part!\n",
    "- Now that you've successfully implemented **CBOW**, we'll use the snapshots recorded during training to visualize how tokens move through the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_embeddings_gif(snapshots_cb, vocab, save_path='cbow.gif', n_words=len(vocab), fps=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b229367",
   "metadata": {},
   "source": [
    "# Let's get Perplexed!\n",
    "- We are going to calculate the perplexity of **CBOW** (check the guidebook for a quick reminder)\n",
    "- Task: try to implement `perplexity_cbow`, what does this number mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b421018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perplexity_cbow(W_in, W_out, examples):\n",
    "    total_nll = 0.0\n",
    "    count = 0\n",
    "    # --- Task begins here ---\n",
    "    \"\"\"Something missing here?\"\"\"\n",
    "    # --- Task ends here ---\n",
    "    return math.exp(total_nll / count) if count > 0 else float('inf')\n",
    "\n",
    "ppl_cb = perplexity_cbow(W_in_cb, W_out_cb, cbow_examples)\n",
    "\n",
    "print(f\"Perplexity using CBOW: {ppl_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3396604",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56368d5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa2328",
   "metadata": {},
   "source": [
    "# Optional Part (a bit hardcore, no torch challenge, totally unnecessary)\n",
    "### Skip-Gram (skipping?)\n",
    "- Now that we have our tokens and vocab (already created for CBOW), we will start creating our simple word2vec model (we will be using the **Skip-gram** architecture for this part)\n",
    "- Tasks:\n",
    "    - Task 1: Implement `generate_skipgram_pairs(tokens, idx, window=2)`. \n",
    "        - Return a list of **(center_idx, context_idx)** tuples where **center_idx = idx\\[token_at_i\\]** and context_idx = **idx\\[token_at_j\\]** for all j in [i-window, i+window] (skip j==i), clipped to sequence bounds. \n",
    "        - Skip tokens not in idx. Keep duplicate pairs (frequency matters!). \n",
    "        - Example: tokens = [\"the\",\"cat\",\"sat\"], idx={\"the\":0,\"cat\":1,\"sat\":2}, window=1 -> [(0,1),(1,0),(1,2),(2,1)].\n",
    "    - Task 2: Now we want to initialize the **input embedding matrix** (`W_in`) and the **output embedding matrix** (`W_out`)\n",
    "        - Hint: check the guidebook if you haven't already\n",
    "    - Task 3: Like we did in previous exercises: we want to implement the forward pass and the loss calculation\n",
    "        - Hint: we will be using the negative log-likelihood loss\n",
    "    - Task 4: Compute the gradients\n",
    "    - Task 5: Update the parameters (the ones we initialized in Task 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(tokens, idx, window=2):\n",
    "    pairs = []\n",
    "    # --- Task 1 begins here ---\n",
    "\n",
    "    # --- Task 1 ends here ---\n",
    "    return pairs\n",
    "\n",
    "def train_skipgram(pairs, vocab_size, embedding_dim=16, lr=0.05, epochs=200, print_every=50, record_every=1):\n",
    "    rng = np.random.RandomState(1)\n",
    "    \n",
    "    # --- Task 2 begins here ---\n",
    "    W_in = \"NOT IMPLEMENTED YET\"\n",
    "    W_out = \"NOT IMPLEMENTED YET\"\n",
    "    # --- Task 2 ends here ---\n",
    "    \n",
    "    # --- Animation Purposes --- Begin ---\n",
    "    snapshots = []\n",
    "    # --- Animation Purposes --- End ---\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        np.random.shuffle(pairs)\n",
    "        total_loss = 0.0\n",
    "        for center, context in pairs:\n",
    "            \n",
    "            # --- Task 3 begins here ---\n",
    "            embed = \"NOT IMPLEMENTED YET\"\n",
    "            scores = \"NOT IMPLEMENTED YET\"\n",
    "            probs = \"NOT IMPLEMENTED YET\"\n",
    "            loss = \"NOT IMPLEMENTED YET\"\n",
    "            total_loss += loss\n",
    "            # --- Task 3 ends here ---\n",
    "\n",
    "            # --- Task 4 begins here ---\n",
    "            grad_scores = \"NOT IMPLEMENTED YET\"\n",
    "            grad_scores[context] = \"NOT IMPLEMENTED YET\"\n",
    "            dW_out = \"NOT IMPLEMENTED YET\"\n",
    "            d_embed = \"NOT IMPLEMENTED YET\"\n",
    "            # --- Task 4 ends here ---\n",
    "\n",
    "            # --- Task 5 begins here ---\n",
    "            W_out = \"NOT IMPLEMENTED YET\"\n",
    "            W_in[center] = \"NOT IMPLEMENTED YET\"\n",
    "            # --- Task 5 ends here ---\n",
    "\n",
    "        # --- Animation Purposes --- Begin ---\n",
    "        if record_every and (ep % record_every == 0 or ep == epochs):\n",
    "            snapshots.append(W_in.copy())\n",
    "        # --- Animation Purposes --- End ---\n",
    "\n",
    "        if ep % print_every == 0 or ep == 1:\n",
    "            print(f\"Epoch {ep}/{epochs}  avg_loss={total_loss/len(pairs):.4f}\")\n",
    "\n",
    "    return W_in, W_out, snapshots\n",
    "\n",
    "\n",
    "# Here we use our tokenization function\n",
    "tokens = tokenize(text)\n",
    "\n",
    "# Here we build our vocabulary and index mapping\n",
    "vocab, idx = build_vocab(tokens)\n",
    "\n",
    "# Here we generate our skip-gram pairs\n",
    "pairs = generate_skipgram_pairs(tokens, idx, window=2)\n",
    "\n",
    "# Let's see some stats\n",
    "print('vocab size:', len(vocab), 'pairs:', len(pairs))\n",
    "\n",
    "# We use our training loop, you can change the parameters as you wish!\n",
    "W_in_sg, W_out_sg, snapshots_sg = train_skipgram(pairs, vocab_size=len(vocab), embedding_dim=16, lr=0.01, epochs=400, print_every=100)\n",
    "\n",
    "# We use our helper function to see nearest neighbors of some words in the learned embedding space\n",
    "for w in ['cat', 'dog', 'mat', 'sat', 'rug']:\n",
    "    if w in idx:\n",
    "        print('\\nNearest to', w, ':', nearest_neighbors(w, vocab, idx, W_in_sg, k=5))\n",
    "    else:\n",
    "        print('\\nWord', w, 'not in vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b4c85",
   "metadata": {},
   "source": [
    "# The Fun Part (Part 2)!\n",
    "- Now that you managed to implement **skip-gram**, we are going to use the snapshots we recorded during the training, to see our tokens move in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna use the function we defined earlier\n",
    "animate_embeddings_gif(snapshots_sg, vocab, save_path='skip_grams.gif', n_words=len(vocab), fps=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd5d6e",
   "metadata": {},
   "source": [
    "#### Skip-Gram Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df351dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_skipgram(W_in, W_out, pairs):\n",
    "    total_nll = 0.0\n",
    "    count = 0\n",
    "    for center, context in pairs:\n",
    "        embed = W_in[center]\n",
    "        scores = W_out.T.dot(embed)\n",
    "        probs = softmax(scores)\n",
    "        total_nll += -np.log(probs[context] + 1e-9)\n",
    "        count += 1\n",
    "    return math.exp(total_nll / count) if count > 0 else float('inf')\n",
    "\n",
    "ppl_sg = perplexity_skipgram(W_in_sg, W_out_sg, pairs)\n",
    "\n",
    "print(f\"\\nPerplexity using skip-gram: {ppl_sg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
