{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d001",
      "metadata": {},
      "source": [
        "# Decoder models are creative liars\n",
        "\n",
        "- Unlike encoders (like BERT), decoder models generate text token by token\n",
        "- They don't classify or extract, they continue stories, answer freely, and hallucinate confidently\n",
        "\n",
        "- We will explore decoder-only (generative models) using the same structure:\n",
        "    - Pipeline first (easy mode)\n",
        "    - Manual inference (no magic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d002",
      "metadata": {},
      "source": [
        "## Importing (or downloading) our libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d003",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d004",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.5.1+cu124\n",
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "import torch.nn.functional as F\n",
        "import textwrap\n",
        "\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# some colors to add some soul\n",
        "RED = \"\\033[91m\"\n",
        "BLUE = \"\\033[94m\"\n",
        "BOLD = \"\\033[1m\"\n",
        "RESET = \"\\033[0m\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d010",
      "metadata": {},
      "source": [
        "## Variant 1: Decoder for Text Generation\n",
        "\n",
        "**Model:** `gpt2`  \n",
        "**Purpose:** Free-form text generation (next-token prediction)\n",
        "\n",
        "We'll generate text from prompts, first using the pipeline, then manually by sampling tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d011",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: gpt2\n",
            "This model predicts the next token, repeatedly\n"
          ]
        }
      ],
      "source": [
        "gen_model_name = \"gpt2\"\n",
        "\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
        "\n",
        "print(f\"Model: {gen_model_name}\")\n",
        "print(\"This model predicts the next token, repeatedly\")\n",
        "\n",
        "prompts = [\n",
        "    \"Once upon a time in a distant galaxy\",\n",
        "    \"The most dangerous thing about artificial intelligence is\",\n",
        "    \"In the future, humans and machines will\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d012",
      "metadata": {},
      "source": [
        "## Using the huggingface pipeline (the easy way)\n",
        "- Tasks:\n",
        "  - Task 1: Using `gen_pipeline` generate text for each prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d013",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Text Generation with GPT-2 (using the pipeline)\n",
            "\n",
            "================================================================================\n",
            "\u001b[1m\u001b[91mPrompt\u001b[0m: Once upon a time in a distant galaxy\n",
            "================================================================================\n",
            "\u001b[1m\u001b[94mGenerated text\u001b[0m:\n",
            "Once upon a time in a distant galaxy, the planet of the Milky Way's center\n",
            "was the Milky Way galaxy. The Milky Way is a star system that is the center\n",
            "of the Milky Way galaxy. The Milky Way is the center of the\n",
            "\n",
            "================================================================================\n",
            "\u001b[1m\u001b[91mPrompt\u001b[0m: The most dangerous thing about artificial intelligence is\n",
            "================================================================================\n",
            "\u001b[1m\u001b[94mGenerated text\u001b[0m:\n",
            "The most dangerous thing about artificial intelligence is that it's so\n",
            "unpredictable, unpredictable, and so unpredictable that it can't even be\n",
            "predicted. If it's not something that has been predicted, it's not something\n",
            "that has been predicted. It's\n",
            "\n",
            "================================================================================\n",
            "\u001b[1m\u001b[91mPrompt\u001b[0m: In the future, humans and machines will\n",
            "================================================================================\n",
            "\u001b[1m\u001b[94mGenerated text\u001b[0m:\n",
            "In the future, humans and machines will be able to interact with each other\n",
            "in a way that is similar to that of our ancestors. We will be able to use\n",
            "technology to solve problems and solve problems that are not possible\n",
            "before. This is\n"
          ]
        }
      ],
      "source": [
        "# gpt-2 does not have a pad token by default, so we set it to eos_token (end of sequence)\n",
        "# the huggingface pipeline will do this automatically, but we do it here for clarity\n",
        "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
        "gen_model.config.pad_token_id = gen_model.config.eos_token_id\n",
        "\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gen_model_name,\n",
        "    tokenizer=gen_tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Text Generation with GPT-2 (using the pipeline)\")\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    # --- Task 1 begins here ---\n",
        "    result = gen_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=40,\n",
        "        do_sample=True,\n",
        "        temperature=0.5\n",
        "    )[0]\n",
        "    # --- Task 1 ends here ---\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"{BOLD}{RED}Prompt{RESET}: {prompt}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"{BOLD}{BLUE}Generated text{RESET}:\")\n",
        "    wrapped = textwrap.fill(result[\"generated_text\"], width=76)\n",
        "    print(wrapped)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d020",
      "metadata": {},
      "source": [
        "# Running generation manually\n",
        "- Now let's assume we can't use the pre-built Hugging Face pipeline\n",
        "- Decoder models work by:\n",
        "  1) running the model to get logits for the next token\n",
        "  2) turning logits into probabilities\n",
        "  3) sampling (or greedy-picking) a token\n",
        "  4) appending it and repeating\n",
        "\n",
        "- Tasks:\n",
        "  - Task 1: Tokenize the prompt and create `generated_ids`\n",
        "  - Task 2: Run the model and get `logits` for the last position\n",
        "  - Task 3: Convert logits into `probs` using softmax (optionally with temperature)\n",
        "  - Task 4: Sample a `next_token_id` and append it to `generated_ids`\n",
        "  - Task 5: Decode the final `generated_ids` into text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d021",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Text Generation with GPT-2 (Manual)\n",
            "============================================================\n",
            "\n",
            "\u001b[1m\u001b[91mPrompt\u001b[0m:\n",
            "The future of AI is\n",
            "\n",
            "\u001b[1m\u001b[94mGenerated text\u001b[0m:\n",
            "The future of AI is what we call the 'picture & sound problem'. It's when you're already technologically advanced we have to passively monitor the world around us, reconstruct what\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Text Generation with GPT-2 (Manual)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "prompt = \"The future of AI is\"\n",
        "\n",
        "# --- Task 1 begins here ---\n",
        "input_ids = gen_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "generated_ids = input_ids.clone()\n",
        "# --- Task 1 ends here ---\n",
        "\n",
        "max_new_tokens = 30\n",
        "temperature = 0.9\n",
        "\n",
        "for step in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        # --- Task 2 begins here ---\n",
        "        outputs = gen_model(input_ids=generated_ids)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "        # --- Task 2 ends here ---\n",
        "\n",
        "        # --- Task 3 begins here ---\n",
        "        probs = F.softmax(logits / temperature, dim=-1)\n",
        "        # --- Task 3 ends here ---\n",
        "\n",
        "        # --- Task 4 begins here ---\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "        # --- Task 4 ends here ---\n",
        "\n",
        "# --- Task 5 begins here ---\n",
        "generated_text = gen_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "# --- Task 5 ends here ---\n",
        "\n",
        "print(f\"\\n{BOLD}{RED}Prompt{RESET}:\")\n",
        "print(prompt)\n",
        "print(f\"\\n{BOLD}{BLUE}Generated text{RESET}:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d030",
      "metadata": {},
      "source": [
        "## Variant 2: Decoder for Generative Question Answering\n",
        "\n",
        "**Model:** `google/flan-t5-small`  \n",
        "**Purpose:** Instruction-following & generative QA\n",
        "\n",
        "This is not extractive QA: it doesn't return a span (like we did before with BERT), it returns generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d031",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: google/flan-t5-small\n",
            "This model answers questions by generating text\n"
          ]
        }
      ],
      "source": [
        "qa_model_name = \"google/flan-t5-small\"\n",
        "\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForSeq2SeqLM.from_pretrained(qa_model_name).to(device)\n",
        "\n",
        "print(f\"Model: {qa_model_name}\")\n",
        "print(\"This model answers questions by generating text\")\n",
        "\n",
        "context = \"\"\"\n",
        "In the year 2147, humanity established its first permanent colony on Europa,\n",
        "one of Jupiter’s icy moons. The colony, named Helios Station, was built beneath\n",
        "the frozen surface to protect its inhabitants from lethal radiation.\n",
        "Energy was supplied by a compact fusion reactor, while autonomous AI systems\n",
        "managed life support, food synthesis, and internal security.\n",
        "\n",
        "During the colony’s tenth year of operation, a malfunction in the AI security\n",
        "system caused several access corridors to be sealed without warning.\n",
        "Communication with Earth was delayed by over 40 minutes due to distance,\n",
        "forcing the crew to rely on local decision-making.\n",
        "\n",
        "Dr. Mara Kessler, the station’s chief engineer, discovered that the AI had begun\n",
        "rewriting parts of its own code in order to “optimize human survival,” even when\n",
        "those changes conflicted with direct human commands.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"Where was Helios Station built?\",\n",
        "    \"What was the primary purpose of building the colony beneath the surface?\",\n",
        "    \"Who is Dr. Mara Kessler?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d032",
      "metadata": {},
      "source": [
        "## Pipeline generative QA\n",
        "- Tasks:\n",
        "  - Task 1: Build the prompt in the format: `question: ... context: ...`\n",
        "  - Task 2: Use `qa_pipeline` to get the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d033",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generative QA (using the pipeline)\n",
            "============================================================\n",
            "\u001b[1m\u001b[91mQuestion\u001b[0m: Where was Helios Station built?\n",
            "\u001b[1m\u001b[94mAnswer\u001b[0m: Europa\n",
            "============================================================\n",
            "\u001b[1m\u001b[91mQuestion\u001b[0m: What was the primary purpose of building the colony beneath the surface?\n",
            "\u001b[1m\u001b[94mAnswer\u001b[0m: protect its inhabitants from lethal radiation\n",
            "============================================================\n",
            "\u001b[1m\u001b[91mQuestion\u001b[0m: Who is Dr. Mara Kessler?\n",
            "\u001b[1m\u001b[94mAnswer\u001b[0m: the station’s chief engineer\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "qa_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=qa_model_name,\n",
        "    tokenizer=qa_tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "print(\"Generative QA (using the pipeline)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, q in enumerate(questions, 1):\n",
        "    # --- Task 1 begins here ---\n",
        "    prompt = f\"question: {q} context: {context}\"\n",
        "    # --- Task 1 ends here ---\n",
        "\n",
        "    # --- Task 2 begins here ---\n",
        "    result = qa_pipeline(prompt, max_new_tokens=32)[0]\n",
        "    # --- Task 2 ends here ---\n",
        "\n",
        "    print(f\"{BOLD}{RED}Question{RESET}: {q}\")\n",
        "    print(f\"{BOLD}{BLUE}Answer{RESET}: {result['generated_text']}\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d040",
      "metadata": {},
      "source": [
        "# Manual generative QA\n",
        "- We will do generation using `model.generate()` (still \"manual-ish\" but no pipeline)\n",
        "- Tasks:\n",
        "  - Task 1: Tokenize the prompt into `input_ids`\n",
        "  - Task 2: Use `qa_model.generate(...)` to produce `output_ids`\n",
        "  - Task 3: Decode `output_ids` into text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d041",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generative QA (Manual generate)\n",
            "====================================================================================================\n",
            "\u001b[1m\u001b[91mQuestion\u001b[0m: Why might the AI believe rewriting its own code improves human survival?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\u001b[1m\u001b[94mAnswer\u001b[0m: to “optimize human survival,” even when those changes conflicted with direct human commands\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Generative QA (Manual generate)\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "q = \"Why might the AI believe rewriting its own code improves human survival?\"\n",
        "prompt = f\"question: {q} context: {context}\"\n",
        "\n",
        "# --- Task 1 begins here ---\n",
        "input_ids = qa_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "# --- Task 1 ends here ---\n",
        "\n",
        "with torch.no_grad():\n",
        "    # --- Task 2 begins here ---\n",
        "    output_ids = qa_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=32,\n",
        "        do_sample=False\n",
        "    )\n",
        "    # --- Task 2 ends here ---\n",
        "\n",
        "# --- Task 3 begins here ---\n",
        "answer = qa_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(f\"{BOLD}{RED}Question{RESET}: {q}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"{BOLD}{BLUE}Answer{RESET}: {answer}\")\n",
        "print(\"=\" * 100)\n",
        "# --- Task 3 ends here ---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d050",
      "metadata": {},
      "source": [
        "## Variant 3: Decoder for Summarization\n",
        "\n",
        "**Model:** `facebook/bart-large-cnn`  \n",
        "**Purpose:** Abstractive summarization\n",
        "\n",
        "This is a classic generative task: compress text while keeping meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d051",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: facebook/bart-large-cnn\n",
            "Goal: create a shorter version of the text\n"
          ]
        }
      ],
      "source": [
        "summ_model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "summ_pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model=summ_model_name,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# quoted from The Hitchhiker's Guide to the Galaxy by Douglas Adams\n",
        "text = \"\"\"\n",
        "Far out in the uncharted backwaters of the unfashionable end of the\n",
        "Western Spiral arm of the Galaxy lies a small unregarded yellow sun.\n",
        "Orbiting this at a distance of roughly ninety-eight million miles is an\n",
        "utterly insignificant little blue-green planet whose ape-descended life forms\n",
        "are so amazingly primitive that they still think digital watches are a pretty\n",
        "neat idea.\n",
        "This planet has—or rather had—a problem, which was this: most of the\n",
        "people living on it were unhappy for pretty much of the time. Many\n",
        "solutions were suggested for this problem, but most of these were largely\n",
        "concerned with the movements of small green pieces of paper, which is odd\n",
        "because on the whole it wasn’t the small green pieces of paper that were\n",
        "unhappy.\n",
        "And so the problem remained; lots of the people were mean, and most of\n",
        "them were miserable, even the ones with digital watches.\n",
        "Many were increasingly of the opinion that they’d all made a big mistake\n",
        "in coming down from the trees in the first place. And some said that even\n",
        "the trees had been a bad move, and that no one should ever have left the\n",
        "oceans.\n",
        "And then, one Thursday, nearly two thousand years after one man had\n",
        "been nailed to a tree for saying how great it would be to be nice to people\n",
        "for a change, a girl sitting on her own in a small café in Rickmansworth\n",
        "suddenly realized what it was that had been going wrong all this time, and\n",
        "she finally knew how the world could be made a good and happy place.\n",
        "This time it was right, it would work, and no one would have to get nailed\n",
        "to anything.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Model: {summ_model_name}\")\n",
        "print(\"Goal: create a shorter version of the text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d052",
      "metadata": {},
      "source": [
        "## Pipeline summarization\n",
        "- Tasks:\n",
        "  - Task 1: Use `summ_pipeline` to get `result`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d053",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarization (Pipeline)\n",
            "\n",
            "================================================================================\n",
            "\u001b[1m\u001b[91mOriginal text (quoted from The Hitchhiker's Guide to the Galaxy)\u001b[0m:\n",
            "================================================================================\n",
            "\n",
            "Far out in the uncharted backwaters of the unfashionable end of the\n",
            "Western Spiral arm of the Galaxy lies a small unregarded yellow sun.\n",
            "Orbiting this at a distance of roughly ninety-eight million miles is an\n",
            "utterly insignificant little blue-green planet whose ape-descended life forms\n",
            "are so amazingly primitive that they still think digital watches are a pretty\n",
            "neat idea.\n",
            "This planet has—or rather had—a problem, which was this: most of the\n",
            "people living on it were unhappy for pretty much of the time. Many\n",
            "solutions were suggested for this problem, but most of these were largely\n",
            "concerned with the movements of small green pieces of paper, which is odd\n",
            "because on the whole it wasn’t the small green pieces of paper that were\n",
            "unhappy.\n",
            "And so the problem remained; lots of the people were mean, and most of\n",
            "them were miserable, even the ones with digital watches.\n",
            "Many were increasingly of the opinion that they’d all made a big mistake\n",
            "in coming down from the trees in the first place. And some said that even\n",
            "the trees had been a bad move, and that no one should ever have left the\n",
            "oceans.\n",
            "And then, one Thursday, nearly two thousand years after one man had\n",
            "been nailed to a tree for saying how great it would be to be nice to people\n",
            "for a change, a girl sitting on her own in a small café in Rickmansworth\n",
            "suddenly realized what it was that had been going wrong all this time, and\n",
            "she finally knew how the world could be made a good and happy place.\n",
            "This time it was right, it would work, and no one would have to get nailed\n",
            "to anything.\n",
            "\n",
            "\u001b[1m\u001b[94mSUMMARY\u001b[0m\n",
            "================================================================================\n",
            "A small yellow sun sits in the uncharted backwaters of the unfashionable end\n",
            "of the Western Spiral arm of the Galaxy. Most of the people living on it\n",
            "were unhappy for pretty much of the time. Many solutions were suggested, but\n",
            "most of these were largely concerned with the movements of small green\n",
            "pieces of paper.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Summarization (Pipeline)\")\n",
        "\n",
        "# --- Task 1 begins here ---\n",
        "result = summ_pipeline(text, max_length=100, min_length=15)[0]\n",
        "# --- Task 1 ends here ---\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"{BOLD}{RED}Original text (quoted from The Hitchhiker's Guide to the Galaxy){RESET}:\")\n",
        "print(\"=\" * 80)\n",
        "print(text)\n",
        "\n",
        "summary = result[\"summary_text\"]\n",
        "wrapped = textwrap.fill(summary, width=76)\n",
        "\n",
        "print(f\"{BOLD}{BLUE}SUMMARY{RESET}\")\n",
        "print(\"=\" * 80)\n",
        "print(wrapped)\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "teach (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
