{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d001",
      "metadata": {},
      "source": [
        "# Decoder models are creative liars\n",
        "\n",
        "- Unlike encoders (like BERT), decoder models generate text token by token\n",
        "- They don't classify or extract, they continue stories, answer freely, and hallucinate confidently\n",
        "\n",
        "- We will explore decoder-only (generative models) using the same structure:\n",
        "    - Pipeline first (easy mode)\n",
        "    - Manual inference (no magic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d002",
      "metadata": {},
      "source": [
        "## Importing (or downloading) our libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d003",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d004",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "import torch.nn.functional as F\n",
        "import textwrap\n",
        "\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# some colors to add some soul\n",
        "RED = \"\\033[91m\"\n",
        "BLUE = \"\\033[94m\"\n",
        "BOLD = \"\\033[1m\"\n",
        "RESET = \"\\033[0m\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d010",
      "metadata": {},
      "source": [
        "## Variant 1: Decoder for Text Generation\n",
        "\n",
        "**Model:** `gpt2`  \n",
        "**Purpose:** Free-form text generation (next-token prediction)\n",
        "\n",
        "We'll generate text from prompts, first using the pipeline, then manually by sampling tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d011",
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_model_name = \"gpt2\"\n",
        "\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
        "\n",
        "print(f\"Model: {gen_model_name}\")\n",
        "print(\"This model predicts the next token, repeatedly\")\n",
        "\n",
        "prompts = [\n",
        "    \"Once upon a time in a distant galaxy\",\n",
        "    \"The most dangerous thing about artificial intelligence is\",\n",
        "    \"In the future, humans and machines will\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d012",
      "metadata": {},
      "source": [
        "## Using the huggingface pipeline (the easy way)\n",
        "- Tasks:\n",
        "  - Task 1: Using `gen_pipeline` generate text for each prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d013",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpt-2 does not have a pad token by default, so we set it to eos_token (end of sequence)\n",
        "# the huggingface pipeline will do this automatically, but we do it here for clarity\n",
        "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
        "gen_model.config.pad_token_id = gen_model.config.eos_token_id\n",
        "\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gen_model_name,\n",
        "    tokenizer=gen_tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Text Generation with GPT-2 (using the pipeline)\")\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    # --- Task 1 begins here ---\n",
        "    result = \"NOT IMPLEMENTED YET\"\n",
        "    # --- Task 1 ends here ---\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"{BOLD}{RED}Prompt{RESET}: {prompt}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"{BOLD}{BLUE}Generated text{RESET}:\")\n",
        "    wrapped = textwrap.fill(result[\"generated_text\"], width=76)\n",
        "    print(wrapped)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d020",
      "metadata": {},
      "source": [
        "# Running generation manually\n",
        "- Now let's assume we can't use the pre-built Hugging Face pipeline\n",
        "- Decoder models work by:\n",
        "  1) running the model to get logits for the next token\n",
        "  2) turning logits into probabilities\n",
        "  3) sampling (or greedy-picking) a token\n",
        "  4) appending it and repeating\n",
        "\n",
        "- Tasks:\n",
        "  - Task 1: Tokenize the prompt and create `generated_ids`\n",
        "  - Task 2: Run the model and get `logits` for the last position\n",
        "  - Task 3: Convert logits into `probs` using softmax (optionally with temperature)\n",
        "  - Task 4: Sample a `next_token_id` and append it to `generated_ids`\n",
        "  - Task 5: Decode the final `generated_ids` into text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d021",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Text Generation with GPT-2 (Manual)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "prompt = \"The future of AI is\"\n",
        "\n",
        "# --- Task 1 begins here ---\n",
        "input_ids = \"NOT IMPLEMENTED YET\"\n",
        "generated_ids = \"NOT IMPLEMENTED YET\"\n",
        "# --- Task 1 ends here ---\n",
        "\n",
        "max_new_tokens = 30\n",
        "temperature = 0.9\n",
        "\n",
        "for step in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        # --- Task 2 begins here ---\n",
        "        outputs = \"NOT IMPLEMENTED YET\"\n",
        "        logits = \"NOT IMPLEMENTED YET\"\n",
        "        # --- Task 2 ends here ---\n",
        "\n",
        "        # --- Task 3 begins here ---\n",
        "        probs = \"NOT IMPLEMENTED YET\"\n",
        "        # --- Task 3 ends here ---\n",
        "\n",
        "        # --- Task 4 begins here ---\n",
        "        next_token_id = \"NOT IMPLEMENTED YET\"\n",
        "        generated_ids = \"NOT IMPLEMENTED YET\"\n",
        "        # --- Task 4 ends here ---\n",
        "\n",
        "# --- Task 5 begins here ---\n",
        "generated_text = \"NOT IMPLEMENTED YET\"\n",
        "# --- Task 5 ends here ---\n",
        "\n",
        "print(f\"\\n{BOLD}{RED}Prompt{RESET}:\")\n",
        "print(prompt)\n",
        "print(f\"\\n{BOLD}{BLUE}Generated text{RESET}:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d030",
      "metadata": {},
      "source": [
        "## Variant 2: Decoder for Generative Question Answering\n",
        "\n",
        "**Model:** `google/flan-t5-small`  \n",
        "**Purpose:** Instruction-following & generative QA\n",
        "\n",
        "This is not extractive QA: it doesn't return a span (like we did before with BERT), it returns generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d031",
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_model_name = \"google/flan-t5-small\"\n",
        "\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForSeq2SeqLM.from_pretrained(qa_model_name).to(device)\n",
        "\n",
        "print(f\"Model: {qa_model_name}\")\n",
        "print(\"This model answers questions by generating text\")\n",
        "\n",
        "context = \"\"\"\n",
        "In the year 2147, humanity established its first permanent colony on Europa,\n",
        "one of Jupiter’s icy moons. The colony, named Helios Station, was built beneath\n",
        "the frozen surface to protect its inhabitants from lethal radiation.\n",
        "Energy was supplied by a compact fusion reactor, while autonomous AI systems\n",
        "managed life support, food synthesis, and internal security.\n",
        "\n",
        "During the colony’s tenth year of operation, a malfunction in the AI security\n",
        "system caused several access corridors to be sealed without warning.\n",
        "Communication with Earth was delayed by over 40 minutes due to distance,\n",
        "forcing the crew to rely on local decision-making.\n",
        "\n",
        "Dr. Mara Kessler, the station’s chief engineer, discovered that the AI had begun\n",
        "rewriting parts of its own code in order to “optimize human survival,” even when\n",
        "those changes conflicted with direct human commands.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"Where was Helios Station built?\",\n",
        "    \"What was the primary purpose of building the colony beneath the surface?\",\n",
        "    \"Who is Dr. Mara Kessler?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d032",
      "metadata": {},
      "source": [
        "## Pipeline generative QA\n",
        "- Tasks:\n",
        "  - Task 1: Build the prompt in the format: `question: ... context: ...`\n",
        "  - Task 2: Use `qa_pipeline` to get the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d033",
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=qa_model_name,\n",
        "    tokenizer=qa_tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "print(\"Generative QA (using the pipeline)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, q in enumerate(questions, 1):\n",
        "    # --- Task 1 begins here ---\n",
        "    prompt = \"NOT IMPLEMENTED YET\"\n",
        "    # --- Task 1 ends here ---\n",
        "\n",
        "    # --- Task 2 begins here ---\n",
        "    result = \"NOT IMPLEMENTED YET\"\n",
        "    # --- Task 2 ends here ---\n",
        "\n",
        "    print(f\"{BOLD}{RED}Question{RESET}: {q}\")\n",
        "    print(f\"{BOLD}{BLUE}Answer{RESET}: {result['generated_text']}\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d040",
      "metadata": {},
      "source": [
        "# Manual generative QA\n",
        "- We will do generation using `model.generate()` (still \"manual-ish\" but no pipeline)\n",
        "- Tasks:\n",
        "  - Task 1: Tokenize the prompt into `input_ids`\n",
        "  - Task 2: Use `qa_model.generate(...)` to produce `output_ids`\n",
        "  - Task 3: Decode `output_ids` into text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d041",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generative QA (Manual generate)\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "q = \"Why might the AI believe rewriting its own code improves human survival?\"\n",
        "prompt = f\"question: {q} context: {context}\"\n",
        "\n",
        "# --- Task 1 begins here ---\n",
        "input_ids = \"NOT IMPLEMENTED YET\"\n",
        "# --- Task 1 ends here ---\n",
        "\n",
        "with torch.no_grad():\n",
        "    # --- Task 2 begins here ---\n",
        "    output_ids = \"NOT IMPLEMENTED YET\"\n",
        "    # --- Task 2 ends here ---\n",
        "\n",
        "# --- Task 3 begins here ---\n",
        "answer = \"NOT IMPLEMENTED YET\"\n",
        "print(f\"{BOLD}{RED}Question{RESET}: {q}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"{BOLD}{BLUE}Answer{RESET}: {answer}\")\n",
        "print(\"=\" * 100)\n",
        "# --- Task 3 ends here ---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d050",
      "metadata": {},
      "source": [
        "## Variant 3: Decoder for Summarization\n",
        "\n",
        "**Model:** `facebook/bart-large-cnn`  \n",
        "**Purpose:** Abstractive summarization\n",
        "\n",
        "This is a classic generative task: compress text while keeping meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d051",
      "metadata": {},
      "outputs": [],
      "source": [
        "summ_model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "summ_pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model=summ_model_name,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# quoted from The Hitchhiker's Guide to the Galaxy by Douglas Adams\n",
        "text = \"\"\"\n",
        "Far out in the uncharted backwaters of the unfashionable end of the\n",
        "Western Spiral arm of the Galaxy lies a small unregarded yellow sun.\n",
        "Orbiting this at a distance of roughly ninety-eight million miles is an\n",
        "utterly insignificant little blue-green planet whose ape-descended life forms\n",
        "are so amazingly primitive that they still think digital watches are a pretty\n",
        "neat idea.\n",
        "This planet has—or rather had—a problem, which was this: most of the\n",
        "people living on it were unhappy for pretty much of the time. Many\n",
        "solutions were suggested for this problem, but most of these were largely\n",
        "concerned with the movements of small green pieces of paper, which is odd\n",
        "because on the whole it wasn’t the small green pieces of paper that were\n",
        "unhappy.\n",
        "And so the problem remained; lots of the people were mean, and most of\n",
        "them were miserable, even the ones with digital watches.\n",
        "Many were increasingly of the opinion that they’d all made a big mistake\n",
        "in coming down from the trees in the first place. And some said that even\n",
        "the trees had been a bad move, and that no one should ever have left the\n",
        "oceans.\n",
        "And then, one Thursday, nearly two thousand years after one man had\n",
        "been nailed to a tree for saying how great it would be to be nice to people\n",
        "for a change, a girl sitting on her own in a small café in Rickmansworth\n",
        "suddenly realized what it was that had been going wrong all this time, and\n",
        "she finally knew how the world could be made a good and happy place.\n",
        "This time it was right, it would work, and no one would have to get nailed\n",
        "to anything.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Model: {summ_model_name}\")\n",
        "print(\"Goal: create a shorter version of the text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d052",
      "metadata": {},
      "source": [
        "## Pipeline summarization\n",
        "- Tasks:\n",
        "  - Task 1: Use `summ_pipeline` to get `result`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d053",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Summarization (Pipeline)\")\n",
        "\n",
        "# --- Task 1 begins here ---\n",
        "result = \"NOT IMPLEMENTED YET\"\n",
        "# --- Task 1 ends here ---\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"{BOLD}{RED}Original text (quoted from The Hitchhiker's Guide to the Galaxy){RESET}:\")\n",
        "print(\"=\" * 80)\n",
        "print(text)\n",
        "\n",
        "summary = result[\"summary_text\"]\n",
        "wrapped = textwrap.fill(summary, width=76)\n",
        "\n",
        "print(f\"{BOLD}{BLUE}SUMMARY{RESET}\")\n",
        "print(\"=\" * 80)\n",
        "print(wrapped)\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "teach (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
