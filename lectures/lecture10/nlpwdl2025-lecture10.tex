% !TeX program = lualatex
% !BIB program = biber
% Lualatex is important to render TTF fonts; with pdflatex it's just the regular one
% ratio 16:9 -- https://tex.stackexchange.com/questions/14336/

% compile two versions, inspired by https://tex.stackexchange.com/a/1501
% use the script "compile-pdf.sh"
\newif\ifhandout
% if flags.tex does not exist, create an empty file to be able to compile in TeXstudio
\input{flags}

\ifhandout
\documentclass[12pt,aspectratio=169,handout]{beamer}
\else
\documentclass[12pt,aspectratio=169]{beamer}
\fi


\input{header-include.tex}



\title{Natural Language Processing with Deep Learning}
\subtitle{Lecture 10 --- Evolution of transformer models}
\date{January 8, 2026}
\author{Prof.\ Dr.\ Ivan Habernal}
\institute{
\texttt{www.trusthlt.org} \\
Trustworthy Human Language Technologies Group (TrustHLT) \\
Ruhr University Bochum \& Research Center Trustworthy Data Science and Security}



\begin{document}

\maketitle

\begin{frame}{Last lecture}
Decoder-only models: GPT (General Pre-trained Transformer)
\end{frame}


\begin{frame}{Motivation}

Knowing encoder transformer (BERT) and decoder transformer (GPT), let's go back to the origins


\includegraphics[width=0.7\linewidth]{img/attn1.png}


\begin{tikzpicture}[overlay, remember picture]
\node at (current page.north east)[ref] {
\fullcite{Vaswani.et.al.2017} \par};
\end{tikzpicture}

	
\end{frame}


\begin{frame}
	
	\includegraphics[width=0.50\linewidth]{img/transformer.png}
	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Vaswani.et.al.2017} \par};
	\end{tikzpicture}
	
	
\end{frame}

\begin{frame}{Transformer}

The Transformer uses multi-head attention in three different ways:

(1) In "encoder-decoder attention" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models


\end{frame}

\begin{frame}{Transformer}
	
	The Transformer uses multi-head attention in three different ways:
	
	
	(2) The encoder contains self-attention layers. In a self-attention layer all of the keys, values
	and queries come from the same place, in this case, the output of the previous layer in the
	encoder. Each position in the encoder can attend to all positions in the previous layer of the
	encoder.

	
\end{frame}

\begin{frame}{Transformer}
	
	The Transformer uses multi-head attention in three different ways:
	
	(3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
	all positions in the decoder up to and including that position. We need to prevent leftward
	information ﬂow in the decoder to preserve the auto-regressive property. We implement this
	inside of scaled dot-product attention by masking out (setting to −∞) all values in the input
	of the softmax which correspond to illegal connections.
	
\end{frame}

\begin{frame}{Transformer -- the task}

We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-
target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary.


\begin{tikzpicture}[overlay, remember picture]
	\node at (current page.north east)[ref] {
		\fullcite{Vaswani.et.al.2017} \par};
\end{tikzpicture}

\end{frame}


\begin{frame}{Transformer -- results}
	
\includegraphics[width=\linewidth]{img/wmt1.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Vaswani.et.al.2017} \par};
	\end{tikzpicture}
	
\end{frame}


\section{Every task is a text-to-text task}



\begin{frame}{T5}

"The basic idea underlying our work is to treat every text processing problem as a
“text-to-text” problem, i.e. taking text as input and producing new text as output."


	\includegraphics[width=0.6\linewidth]{img/raffel1.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Raffel.et.al.2020.JMLR} \par};
	\end{tikzpicture}
	
\end{frame}




\begin{frame}{T5}
	
	\includegraphics[width=\linewidth]{img/raffel2.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Raffel.et.al.2020.JMLR} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{T5 --- self-supervised pre-training}
	
	\includegraphics[width=\linewidth]{img/raffel5.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Raffel.et.al.2020.JMLR} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{T5 -- Source data quality matters}
	
"Common Crawl is a publicly-available web archive that provides “web extracted text”
by removing markup and other non-text content from the scraped HTML ﬁles. This process
produces around 20TB of scraped text data each month. Unfortunately, the majority of the
resulting text is not natural language."




	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Raffel.et.al.2020.JMLR} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{T5 -- Colossal Clean Common Crawl corpus (about 750 GB)}
	
\includegraphics[width=0.9\linewidth]{img/raffel3.png}	
	
	
	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Raffel.et.al.2020.JMLR} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{T5 -- Scale matters the most}

"scaling the model size to 11 billion parameters was the most important ingredient for achieving our best performance."
	
\includegraphics[width=0.6\linewidth]{img/raffel4.png}	
	
	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Raffel.et.al.2020.JMLR} \par};
	\end{tikzpicture}
	
\end{frame}


\section{Evolution of GPT}
	

\begin{frame}{Towards GPT-1}
	
Decoder part of the Transformer Encoder-Decoder model for MT \citep{Vaswani.et.al.2017}

Dropping encoder and using only decoder that consumes input and produces output trained as a standard language model for writing Wikipedia pages as summarization task \citep{Liu.et.al.2018.ICLR}


	
\begin{tikzpicture}[overlay, remember picture]
\node at (current page.north east)[ref] {
\fullcite{Vaswani.et.al.2017} \newline
\fullcite{Liu.et.al.2018.ICLR} \par};
\end{tikzpicture}


\end{frame}


\begin{frame}{GPT-1}

GPT-1 \citep{Radford.et.al.2018.GPT1.report} adapted decoder only transformer

\begin{itemize}
\item pre-training as LM
\item fine-tuning with an extra final layer for the given task
\item pre-trained on BooksCorpus (7k unique unpublished books)
\item 12 decoder layers, 12 attention heads, 768 embedding size
\end{itemize}


\emph{``improving the state of the art on 9 of the 12 datasets we study"}	
	
	
\begin{tikzpicture}[overlay, remember picture]
\node at (current page.north east)[ref] {
\fullcite{Radford.et.al.2018.GPT1.report} \par};
\end{tikzpicture}
	
	
\end{frame}


\begin{frame}{GPT-2}
	
Larger GPT-1
	
	\begin{itemize}
		\item pre-training as LM
		\item pre-trained on custom web scrape (all outbounds links from Reddit with at least 3 karma points, for quality reasons), 8 million documents total
		\item 48 decoder layers, 1600 embedding size (1.542 billion params)
	\end{itemize}

Representing inputs, prompting, etc. --- next lectures
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Radford.et.al.2019.GPT2.report} \par};
	\end{tikzpicture}
	
	
\end{frame}


\section{In-context learning}



\begin{frame}{What is in-context-learning}

``In-context learning (ICL) is the ability of a model to \textbf{use inputs} at \textbf{inference time} to \textbf{adapt its behavior}, \textbf{without weight updates}, in order to solve tasks not present during training."

\bigskip

``Brown et al. (2020) first observed that a transformer-based language model, GPT-3, trained auto-regressively at sufficient scale, exhibited ICL without any specific effort of the authors to promote it via the training objective or data."


\begin{tikzpicture}[overlay, remember picture]
\node at (current page.north east)[ref] {
\fullcite{Singh.et.al.2023.NeurIPS} \par};
\end{tikzpicture}


\end{frame}


\begin{frame}


\includegraphics[width=0.55\linewidth]{img/gpt1.png}	



\begin{tikzpicture}[overlay, remember picture]
	\node at (current page.north east)[ref] {
		\fullcite{Brown.et.al.2020.GPT3} \par};
\end{tikzpicture}
	
\end{frame}


\begin{frame}{GPT-3}
	
	\includegraphics[width=0.7\linewidth]{img/gpt2.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{GPT-3}
	
	\includegraphics[width=0.7\linewidth]{img/gpt3.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{GPT-3}
	
	\includegraphics[width=0.7\linewidth]{img/gpt4.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{GPT-3 Pre-training data}
	
	\includegraphics[width=1.0\linewidth]{img/gpt5.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{GPT-3 Some results}
	
	\includegraphics[width=0.9\linewidth]{img/gpt6.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{GPT-3 Some results}
	
	\includegraphics[width=0.6\linewidth]{img/gpt7.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{GPT-3 large model generates plausible new articles}
	
	\includegraphics[width=\linewidth]{img/gpt8.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Brown.et.al.2020.GPT3} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{Why does in-context learning work?}
	
Active area of research!

"we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3."

"Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence."
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Min.et.al.2022.EMNLP} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{Why does in-context learning work?}
	
"we explain language models as metaoptimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent."
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Dai.et.al.2023.ACLFindings} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{In-context learning take-aways}

``GPT3 paper showed that a non-trivial alternative to fine-tuning emerges when the LM is large enough: an LM can be specialized to a downstream NLP task by simply receiving in its input a string composed of concatenated training examples of this task"

\begin{itemize}
\item While the LM's weights are unchanged in this procedure, some form of learning evidently takes place
\item The performance significantly improves with the number of concatenated training examples, for a disparate variety of NLP tasks
\end{itemize}


\begin{tikzpicture}[overlay, remember picture]
\node at (current page.north east)[ref] {
\fullcite{Wies.et.al.2023.NeurIPS} \par};
\end{tikzpicture}

\end{frame}


\begin{frame}{In-context learning take-aways}

ICL has had a profound practical impact on the applicability of large LMs
\begin{itemize}
\item No need to have any access to the model weights in order to specialize the model for a certain task
\item Instead, a string of training examples provided even via API access to the model is enough
\item Often not many examples are required
\end{itemize}

\begin{tikzpicture}[overlay, remember picture]
\node at (current page.north east)[ref] {
\fullcite{Wies.et.al.2023.NeurIPS} \par};
\end{tikzpicture}

\end{frame}


\section{"Alignment", instruction-tuning, RLHF}


\begin{frame}{Llama2}
	
	\includegraphics[width=\linewidth]{img/llama21.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Touvron.et.al.2023.llama2} \par};
	\end{tikzpicture}
	
\end{frame}

\begin{frame}{Llama2}
	

"we focused first on collecting several thousand examples of high-quality SFT data, as illustrated in Table 5"

"We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations."
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Touvron.et.al.2023.llama2} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{Llama2, Reinforcement Learning with Human Feedback (RLHF)}
	
RLHF is a model training procedure that is applied to a fine-tuned language model to further align model behavior with human preferences and instruction following.

"We collect data that represents empirically sampled human preferences, whereby human annotators select which of two model outputs they prefer. This human feedback is subsequently used to train a reward model, which learns patterns in the preferences of the human annotators and can then automate preference decisions."
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Touvron.et.al.2023.llama2} \par};
	\end{tikzpicture}
	
\end{frame}



\begin{frame}{Llama2, Reinforcement Learning with Human Feedback (RLHF)}
	
"Our annotation procedure proceeds as follows. We ask annotators to first write a prompt, then choose between two sampled model responses, based on provided criteria. In order to maximize the diversity, the two responses to a given prompt are sampled from two different model variants, and varying the temperature hyper-parameter. In addition to giving participants a forced choice, we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either their choice is significantly better, better, slightly better, or negligibly better/ unsure."
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Touvron.et.al.2023.llama2} \par};
	\end{tikzpicture}
	
\end{frame}


\begin{frame}{Reinforcement Learning with Human Feedback (RLHF)}
	
	\includegraphics[width=0.9\linewidth]{img/rlhf.png}	
	
	\begin{tikzpicture}[overlay, remember picture]
		\node at (current page.north east)[ref] {
			\fullcite{Ouyang.et.al.2022.NeurIPS} \par};
	\end{tikzpicture}
	
\end{frame}








\begin{frame}{License and credits}

	\begin{columns}
		\begin{column}{0.7\textwidth}
			Licensed under Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)
		\end{column}
		\begin{column}{0.2\textwidth}
			\includegraphics[width=0.9\linewidth]{img/cc-by-sa-icon.pdf}
		\end{column}
	\end{columns}
	
	\bigskip
	
	Credits
	
	\begin{scriptsize}
		
		Ivan Habernal
		
		Content from ACL Anthology papers licensed under CC-BY \url{https://www.aclweb.org/anthology}
		
	
	\end{scriptsize}
	
\end{frame}


\end{document}

