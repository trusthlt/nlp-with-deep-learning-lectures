% !TeX program = lualatex
% !BIB program = biber
% Lualatex is important to render TTF fonts; with pdflatex it's just the regular one
% ratio 16:9 -- https://tex.stackexchange.com/questions/14336/

% compile two versions, inspired by https://tex.stackexchange.com/a/1501
% use the script "compile-pdf.sh"
\newif\ifhandout
% if flags.tex does not exist, create an empty file to be able to compile in TeXstudio
\input{flags}

\ifhandout
\documentclass[12pt,aspectratio=169,handout]{beamer}
\else
\documentclass[12pt,aspectratio=169]{beamer}
\fi



% TODO change "leftfootertext" to your liking
\newcommand{\leftfootertext}{\insertsubtitle}  % just the \title{} text by default
%\newcommand{\leftfootertext}{AI is all you need | Dr.\ Maria Mustermann}  % Your name, for instance


% ------- RUB specifics ----------
% adjust for 16:9
% https://tex.stackexchange.com/questions/354022/modifying-the-margins-of-all-slides-in-beamer
\setbeamersize{text margin left=0.3cm,text margin right=4.5cm} 


% use Metropolis as the basis theme
\usetheme[subsectionpage=progressbar]{metropolis}
% blocks with background globally
\metroset{block=fill}


\usepackage{fontspec}
% RUB fonts need to be installed
% 'UprightFont = * Light' makes sure that the base font is RubFlama Light, which looks
% lighter than RubFlama Regular (would be too thick for slides)
\setsansfont[Scale=MatchLowercase, UprightFont = * Light, BoldFont = * Bold]{RubFlama}
%\setsansfont{Arial} % Open source alternative if you don't have RubFlama

% RUB color scheme
% Dark blue: 0; 53; 96; #003560
\definecolor{RUBDarkBlue}{RGB}{0, 53, 96}

% Light yellow (table fill, etc.); 238; 250; 196; #EEFAC4
\definecolor{RUBLightYellow}{RGB}{238, 250, 196}

%Light green: 141; 174; 16
\definecolor{RUBLightGreen}{RGB}{141, 174, 16}


\setbeamercolor{titlelike}{fg=RUBDarkBlue}
\setbeamercolor{subtitle}{fg=RUBLightGreen}
\setbeamercolor{separation line}{fg=RUBLightGreen}
\setbeamercolor{frametitle}{bg=white, fg=RUBDarkBlue}

% horizontal line on title page and sections
\setbeamercolor{alerted text}{fg=RUBLightGreen}


% Adjust footer bottom (too large by default)
\setbeamertemplate{footline}{%
  \begin{beamercolorbox}[wd=\textwidth, sep=2ex]{footline}%
    \usebeamerfont{page number in head/foot}%
    \usebeamertemplate*{frame footer}
    \hfill%
    \usebeamertemplate*{frame numbering}
  \end{beamercolorbox}%
}


% Lab name, numbering, etc. in footer
\setbeamertemplate{frame numbering}{TrustHLT --- Prof.\ Dr.\ Ivan Habernal \hspace*{1ex} \includegraphics[width=7em]{img/rub-logo.pdf}\hspace*{1ex}}

\setbeamertemplate{frame footer}{\hspace*{1ex}\insertframenumber \hspace*{2ex} \leftfootertext}

% adjust the background to be completely white
\setbeamercolor{background canvas}{bg=white}

% logos on the title page
\titlegraphic{%
	\begin{picture}(0,0)
		\put(435,0){\makebox(0,0)[rt]{\includegraphics[width=7em]{img/rub-logo.pdf}}}
		\put(435,-170){\makebox(0,0)[rt]{\includegraphics[width=4em]{img/logo-trusthlt.pdf}}}
		\put(435,-196){\makebox(0,0)[rt]{\includegraphics[width=9em]{img/logo-rctrust.pdf}}}
	\end{picture}%
}


% show TOC at every section start
\AtBeginSection{
	\frame{
		\vspace{2em}
		\sectionpage
		\hspace*{2.2em}\begin{minipage}{10cm}
			\tableofcontents[currentsection]
		\end{minipage}
	}
}

% TOC without subsection
\setcounter{tocdepth}{1} % only-- part,chapters,sections 

% bullet points: rectangles
\useinnertheme{rectangles}
\setbeamercolor{itemize item}{fg=RUBLightGreen}
\setbeamercolor{itemize subitem}{fg=RUBLightGreen}
% enumerate: blue background for better readability
\setbeamercolor{item projected}{bg=RUBDarkBlue}

% make boxes (example, block, etc.) background lighter for readability
\setbeamercolor{block title}{%
	use=normal text,
	fg=normal text.fg,
	bg=normal text.bg!90!fg % lighter background in block title
}
\setbeamercolor{block body}{
	use={block title, normal text},
	bg=block title.bg!30!normal text.bg % lighter background in block body
}


% RUB colors in blocks
\setbeamercolor{block title alerted}{%
	use={block title, alerted text},
	bg=RUBDarkBlue,
	%fg=RUBLightYellow % looks bad
	fg=white % better contrast
}

\setbeamercolor{block title example}{%
	use={block title, example text},
	fg=RUBLightGreen
}


% ------- end of RUB specifics ----------

% all itemize with pause by default
%\beamerdefaultoverlayspecification{<+->}


% typeset mathematics on serif
\usefonttheme[onlymath]{serif}

% better bibliography using biber as backend
\usepackage[natbib=true,backend=biber,style=authoryear-icomp,maxbibnames=30,maxcitenames=9,uniquelist=false,giveninits=true,doi=false,url=false,dashed=false,isbn=false]{biblatex}
% shared bibliography
\addbibresource{../../nlpwdl-bibliography.bib}
% disable "ibid" for repeated citations
\boolfalse{citetracker}



\usepackage{xspace}


% for derivatives, https://tex.stackexchange.com/a/412442
\usepackage{physics}

\usepackage{tikz}
\usetikzlibrary{matrix, positioning}
\usetikzlibrary{angles,quotes} % for angles
\usetikzlibrary{backgrounds} % background
\usetikzlibrary{decorations.pathreplacing} % curly braces
\usetikzlibrary{calligraphy}
\usetikzlibrary{calc} % for neural nets

% for plotting functions
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

% sub-figures
\usepackage{caption}
\usepackage{subcaption}

% book tabs
\usepackage{booktabs}


% argmin, argmax
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\argmin}{arg\!\min}
% softmax
\DeclareMathOperator*{\softmax}{soft\!\max}
% Mask
\DeclareMathOperator*{\mask}{mask}

% bold math
\usepackage{bm}

% for \mathclap
\usepackage{mathtools}

% algorithms
\usepackage[noend]{algpseudocode}


% for neurons and layers in tikz
\tikzset{
	neuron/.style={draw, rectangle, inner sep=2pt, minimum width=0.75cm, fill=blue!20},
	param/.style={draw, rectangle, inner sep=2pt, minimum width=0.75cm, fill=green!20},
	constant/.style={draw, rectangle, inner sep=2pt, minimum width=0.75cm, fill=black!15},
	% for citation nodes right top
	ref/.style={anchor = north east, text width=7.8cm, yshift=-1.3cm, xshift=-0.2cm, scale=0.5},
	state/.style={rectangle, inner sep=2pt, minimum width=0.75cm, fill=black!5},
}

% added in lecture 10
\tikzset{
	mtx/.style={
		matrix of math nodes,
		left delimiter={[}, right delimiter={]}
	},
	hlt/.style={opacity=0.1, line width=4 mm, line cap=round},
	hltr/.style={opacity=0.5, rounded corners=2pt, inner sep=-1pt}
}

% for strike-through text (added in Lecture 06)
\usepackage[normalem]{ulem}

% added in Lecture 7
% RNN
\DeclareMathOperator*{\rnn}{RNN}
% RNN star
\DeclareMathOperator*{\rnnstar}{RNN^{*}}
% bi-RNN
\DeclareMathOperator*{\birnn}{biRNN}


% added in Lecture 9
\usetikzlibrary{fit} % for hightligting by calling "fit"

% algorithms
\usepackage[noend]{algpseudocode}



\title{Natural Language Processing with Deep Learning}
\subtitle{Lecture 5 --- Feed-forward network and language modeling}
\date{November 14, 2024}
\author{Prof.\ Dr.\ Ivan Habernal}
\institute{
\texttt{www.trusthlt.org} \\
Trustworthy Human Language Technologies Group (TrustHLT) \\
Ruhr University Bochum \& Research Center Trustworthy Data Science and Security}


\begin{document}

\maketitle


\section{From binary to multi-class task}

\begin{frame}{Our binary text classification function}
	
	Linear function through sigmoid --- log-linear model
	$$
	\hat{y} = \sigma(f(\bm{x})) = \frac{1}{1 + \exp(- (\bm{x} \cdot \bm{w} + b))}
\qquad \hat{y} \in (0, 1), y \in \{0, 1\}
	$$	
	
\begin{figure}
	\begin{tikzpicture}	
		%\node (a1) [draw, circle, inner sep=0pt, minimum width=0.75cm, fill=green!20] {$a_1$};
		\node (x) [constant] {$\bm{x}$};
		\node (w) [param, below of=x] {$\bm{w}$};
		\node (b) [param, below of=w] {$b$};
		
		\node (f) [neuron, right of=w, xshift=1.5cm] {$\bm{x} \cdot \bm{w} + b$};
		\node (s) [neuron, right of=f, xshift=1.5cm] {$\sigma$};
		
		\node (l) [neuron, right of=s, xshift=1cm] {$L$};
		\node (y) [constant, below of=s] {$y$};
		
		\begin{scope}[thick, black, ->, >=latex]
			\draw (x) -- (f);
			\draw (w) -- (f);
			\draw (b) -- (f);
			\draw (f) -- (s);
			\draw (s) -- (l) node [midway, above] {$\hat{y}$};
			\draw (y) -- (l);
		\end{scope}	
	\end{tikzpicture}
	\caption{Computational graph; green nodes are trainable parameters, gray are constant inputs}
\end{figure}

	
\end{frame}



\begin{frame}{From binary to multi-class labels}
	
	So far we mapped our gold label $y \in \{0, 1\}$
	
	\begin{itemize}
		\item Categorical: There is no `ordering'
		\item Example: Classify the language of a document into 6 languages (En, Fr, De, It, Es, Other)
	\end{itemize}
	
	\pause
	\begin{block}{One-hot encoding of labels}
		$$
		\begin{aligned}
			\text{En} &= \begin{pmatrix}1 & 0 & 0 & 0 & 0 & 0\end{pmatrix} \qquad
			\text{Fr} = \begin{pmatrix}0 & 1 & 0 & 0 & 0 & 0\end{pmatrix} \\
			\text{De} &= \begin{pmatrix}0 & 0 & 1 & 0 & 0 & 0\end{pmatrix} \qquad \ldots
		\end{aligned}
		$$
	$\bm{y} \in \mathbb{R}^{d_{out}}$ where $d_{out}$ is the number of classes
	\end{block}
	
\end{frame}

\begin{frame}{Possible solution: Six weight vectors and biases}
	
	Consider for each language $\ell \in \{\text{En}, \text{Fr}, \text{De}, \text{It}, \text{Es}, \text{Other}\}$
	\begin{itemize}
		\item Weight vector $\bm{w}^{\ell}$ (e.g., $\bm{w}^{\text{Fr}})$
		\item Bias $b^{\ell}$ (e.g., $b^{\text{Fr}})$
	\end{itemize}
	\pause We can predict the language resulting in the highest score
	$$
	\hat{y} = f(\bm{x}) = \argmax_{
		\ell \in \{\text{En}, \text{Fr}, \text{De}, \text{It}, \text{Es}, \text{Other}\}
	}
	\bm{x} \cdot \bm{w}^{\ell} + b^{\ell}
	$$
	
	\pause
	But we can re-arrange the $\bm{w} \in \mathbb{R}^{d_{in}}$ vectors into columns of a matrix $\bm{W} \in \mathbb{R}^{d_{in} \times 6}$ and $\bm{b} \in \mathbb{R}^6$, to get
	$$f(\bm{x}) = \bm{x} \bm{W} + \bm{b}$$
	
\end{frame}


\begin{frame}{Projecting input vector to output vector $f(\bm{x}) : \mathbb{R}^{d_{in}} \to \mathbb{R}^{d_{out}}$}
	
	\pause
	\begin{block}{Recall from lecture 3: High-dimensional linear functions}
		Function $f(\bm{x}) : \mathbb{R}^{d_{in}} \to \mathbb{R}^{d_{out}}$
		$$f(\bm{x}) = \bm{x} \bm{W} + \bm{b}$$
		where
		$\bm{x} \in \mathbb{R}^{d_{in}} \qquad
		\bm{W} \in \mathbb{R}^{d_{in} \times d_{out}} \qquad
		\bm{b} \in \mathbb{R}^{d_{out}}$
	\end{block}	
	
	
\end{frame}

\begin{frame}{Prediction of multi-class classifier}
	Project the input $\bm{x}$ to an output $\bm{y}$
	$$\bm{\hat{y}} = f(\bm{x}) = \bm{x} \bm{W} + \bm{b}$$
	and pick the element of $\bm{\hat{y}}$ with the highest value
	$$
	\text{prediction} = \hat{y} = \argmax_{i} \bm{\hat{y}}_{[i]}
	$$
	
	\begin{block}{Sanity check}
		What is $\hat{y}$?
		
		\pause
		Index of $1$ in the one-hot. 
		For example, if $\hat{y} = 3$, then the document is in German
		$\text{De} = \begin{pmatrix}0 & 0 & 1 & 0 & 0 & 0\end{pmatrix}$
	\end{block}
	
\end{frame}

\subsection{Representations}


\begin{frame}{Two representations of the input document}
	$$\bm{\hat{y}} = \bm{x} \bm{W} + \bm{b}$$
	
	Vector $\bm{x}$ is a document representation
	\begin{itemize}
		\item Bag of words, for example ($d_{in} = |V|$ dimensions, sparse)
	\end{itemize}
	
	Vector $\bm{\hat{y}}$ is \textbf{also} a document representation
	\begin{itemize}
		\item More compact (only 6 dimensions)
		\item More specialized for the language prediction task
	\end{itemize}
	
\end{frame}


\begin{frame}{Learned representations --- central to deep learning}
	Representations are central to deep learning
	
	One could argue that the main power of deep-learning is the ability to learn good representations
\end{frame}


\subsection{From multi-dimensional linear transformation to probabilities}

\begin{frame}{Turning output vector into probabilities of classes}
	
	\begin{block}{Recap: Categorical probability distribution}
		Categorical random variable $X$ is defined over $K$ categories, typically mapped to natural numbers $1, 2, \ldots, K$, for example En = 1, De = 2, $\ldots$
		
		\pause
		Each category parametrized with probability $\Pr(X = k) = p_k$
		
		\pause
		Must be valid probability distribution: $\sum_{i =1}^{K} \Pr(X = i) = 1$
	\end{block}
	
	\pause
	How to turn an \textbf{unbounded} vector in $\mathbb{R}^K$ into a categorical probability distribution?
	
\end{frame}

\begin{frame}{The softmax function $\softmax (\bm{x}): \mathbb{R}^K \to \mathbb{R}^K$}
	
	\begin{block}{Softmax}
		Applied element-wise, for each element $\bm{x}_{[i]}$ we have
		$$
		\softmax (\bm{x}_{[i]}) = \frac{\exp(\bm{x}_{[i]})}{
			\sum_{k=1}^{K} \exp(\bm{x}_{[k]})
		}
		$$
	\end{block}
	
	\pause
	\begin{itemize}
		\item Nominator: Non-linear bijection from $\mathbb{R}$ to $(0; \infty)$
		\item Denominator: Normalizing constant to ensure $\sum_{j = 1}^{K} \softmax (\bm{x}_{[j]}) = 1$
	\end{itemize}
	
	\pause
	We also need to know how to compute the partial derivative of $\softmax (\bm{x}_{[i]})$ wrt.\ each argument $\bm{x}_{[k]}$: $\pdv{\softmax (\bm{x}_{[i]})}{\bm{x}_{[k]}}$
	
\end{frame}


\begin{frame}{Softmax can be smoothed with a `temperature' $T$}
	\vspace{-1em}
	$$
	\softmax (\bm{x}_{[i]}; T) = \frac{
		\exp(\frac{\bm{x}_{[i]}}{T})
	}{
		\sum_{k=1}^{K} \exp(
		\frac{\bm{x}_{[k]}}{T})
	}
	$$
	
	\pause
	\begin{block}{Example: Softmax of $\bm{x} = (3, 0, 1)$ at different $T$}
		\includegraphics[width=0.90\linewidth]{img/temperatures.png}
		
		High temperature $\to$ uniform distribution
		
		Low temperature $\to$ `spiky' distribution, all mass on the largest element
		
	\end{block}


\begin{tikzpicture}[overlay, remember picture] 
	\node at (current page.north east)[ref] {Figure from \fullcite{Murphy.2012} \par};
\end{tikzpicture}
	
\end{frame}


\section{Loss function for softmax}

\begin{frame}{Categorical cross-entropy loss (aka.\ negative log likelihood)}
	
	Vector representing the gold-standard categorical distribution over the classes/labels $1, \ldots, K$:
	$$
	\bm{y} = (\bm{y_{[1]}}, \bm{y}_{[2]}, \ldots, \bm{y}_{[K]})
	$$
	Output from softmax:
	$$
	\bm{\hat{y}} = (\bm{\hat{y}_{[1]}}, \bm{\hat{y}}_{[2]}, \ldots, \bm{\hat{y}}_{[K]})
	$$
	which is in fact $\bm{\hat{y}_{[i]}} = \Pr(y = i| \bm{x})$
	
	
	\begin{block}{Cross entropy loss}
		$$
		L_{\text{cross-entropy}} (\bm{\hat{y}, \bm{y}}) =
		- \sum_{k = 1}^{K} \bm{y}_{[k]} \log \left(  \bm{\hat{y}}_{[k]} \right)
		$$	
	\end{block}	
\end{frame}



\section{Stacking transformations and non-linearity}


\begin{frame}
	\begin{figure}
		\vspace{-1.3em}
		\includegraphics[width=1.25\linewidth]{img/linear1.png}	
		\caption{Linear model can tackle only linearly-separable problems (\url{http://playground.tensorflow.org})}
	\end{figure}	
\end{frame}

\begin{frame}
	\begin{figure}
		\vspace{-1.3em}
		\includegraphics[width=1.25\linewidth]{img/linear2.png}	
		\caption{Linear model can tackle only linearly-separable problems (\url{http://playground.tensorflow.org})}
	\end{figure}	
\end{frame}


\begin{frame}{Stacking linear layers on top of each other --- still linear!}
	\vspace{-1em}
	$$
	\bm{x} \in \mathbb{R}^{d_{in}} \qquad
	\bm{W^1} \in \mathbb{R}^{d_{in} \times d_1} \qquad
	\bm{b^1} \in \mathbb{R}^{d_1} \qquad
	\bm{W^2} \in \mathbb{R}^{d_1 \times d_{out}} \qquad
	\bm{b^2} \in \mathbb{R}^{d_{out}} \qquad
	$$
	$$
	f(\bm{x}) = \left(
	\bm{x} \bm{W^1} + \bm{b^1}
	\right)
	\bm{W^2} + \bm{b^2}
	$$
	
	\begin{figure}
		\begin{tikzpicture}	
			%\node (a1) [draw, circle, inner sep=0pt, minimum width=0.75cm, fill=green!20] {$a_1$};
			\node (x) [constant] {$\bm{x}$};
			\node (w) [param, below of=x] {$\bm{W^1}$};
			\node (b) [param, below of=w] {$\bm{b^1}$};
			
			\node (f1) [neuron, right of=w, xshift=1.5cm] {$\bm{x} \bm{W^1} + \bm{b^1}$};
			\node (f2) [neuron, right of=f1, xshift=1.5cm] {$\bm{h^1} \bm{W^2} + \bm{b^2}$};
			
			\node (w2) [param, below of=f2, xshift=-1.5cm, yshift=0cm] {$\bm{W^2}$};
			\node (b2) [param, below of=f2, xshift=-0.5cm, yshift=-0.5cm] {$\bm{b^2}$};
			
			\node (l) [neuron, right of=f2, xshift=1cm] {$L$};
			\node (y) [constant, below of=f2, xshift=1.5cm] {$\bm{y}$};
			
			\begin{scope}[thick, black, ->, >=latex]
				\draw (x) -- (f1);
				\draw (w) -- (f1);
				\draw (b) -- (f1);
				\draw (f1) -- (f2);
				\draw (f2) -- (l);
				\draw (w2) -- (f2);
				\draw (b2) -- (f2);
				\draw (y) -- (l);
			\end{scope}	
		\end{tikzpicture}
		\caption{Computational graph; green circles are trainable parameters, gray are constant inputs}
	\end{figure}	
	
\end{frame}


\begin{frame}
	\begin{figure}
		\vspace{-1.3em}
		\includegraphics[width=1.25\linewidth]{img/linear3.png}	
		\caption{Linear hidden layers do not help (\url{http://playground.tensorflow.org})}
	\end{figure}	
\end{frame}




\begin{frame}{Adding non-linear function $g: \mathbb{R}^{d_1} \to \mathbb{R}^{d_1}$}
	\vspace{-1em}
	$$
	f(\bm{x}) = g \left(
	\bm{x} \bm{W^1} + \bm{b^1}
	\right)
	\bm{W^2} + \bm{b^2}
	$$
	
	\begin{figure}
		\begin{tikzpicture}	
			%\node (a1) [draw, circle, inner sep=0pt, minimum width=0.75cm, fill=green!20] {$a_1$};
			\node (x) [constant] {$\bm{x}$};
			\node (w) [param, below of=x] {$\bm{W^1}$};
			\node (b) [param, below of=w] {$\bm{b^1}$};
			
			\node (f1) [neuron, right of=w, xshift=1.5cm] {$\bm{x} \bm{W^1} + \bm{b^1}$};
			
			\node (g) [neuron, right of=f1, xshift=1.5cm] {$g$};
			\node (f2) [neuron, right of=g, xshift=1.5cm] {$\bm{h^1} \bm{W^2} + \bm{b^2}$};
			
			\node (w2) [param, below of=f2, xshift=-1.5cm, yshift=0cm] {$\bm{W^2}$};
			\node (b2) [param, below of=f2, xshift=-0.5cm, yshift=-0.5cm] {$\bm{b^2}$};
			
			\node (l) [neuron, right of=f2, xshift=1cm] {$L$};
			\node (y) [constant, below of=f2, xshift=1.5cm] {$\bm{y}$};
			
			\begin{scope}[thick, black, ->, >=latex]
				\draw (x) -- (f1);
				\draw (w) -- (f1);
				\draw (b) -- (f1);
				\draw (f1) -- (g);
				\draw (g) -- (f2);
				\draw (f2) -- (l);
				\draw (w2) -- (f2);
				\draw (b2) -- (f2);
				\draw (y) -- (l);
			\end{scope}	
		\end{tikzpicture}
		\caption{Computational graph; green circles are trainable parameters, gray are constant inputs}
	\end{figure}	
	
\end{frame}



\begin{frame}{Non-linear function $g$: Rectified linear unit (ReLU) activation}
	
	
	\begin{columns}
		
		\begin{column}{0.6\linewidth}
			
			$$
			\mathrm{ReLU}(z) =
			\begin{cases}
				0  & \quad \text{if } z < 0\\
				z  & \quad \text{if } z \geq 0
			\end{cases}
			$$
			
		or \hspace{0.4em} $\mathrm{ReLU}(z) = \max(0, z)$
			
			
			
			
		\end{column}
		
		\begin{column}{0.4\linewidth}
			\begin{figure}
				\begin{tikzpicture}
					
					\begin{axis}[
						xmin = -5, xmax = 5,
						ymin = -5, ymax = 5,
						xtick distance = 5,
						ytick distance = 5,
						grid = both,
						minor tick num = 5,
						major grid style = {lightgray},
						minor grid style = {lightgray!25},
						width = \textwidth,
						height = \textwidth,
						legend pos = north west
						]
						
						\addplot[
						domain = -5:0,
						samples = 10,
						smooth,
						thick,
						blue,
						] {0};
						
						\addplot[
						domain = 0:5,
						samples = 10,
						smooth,
						thick,
						blue,
						] {x};
						
						
					\end{axis}
					
				\end{tikzpicture}
				\caption{ReLU function}
			\end{figure}
		\end{column}
	\end{columns}
	
	
\end{frame}


\begin{frame}
	\begin{figure}
		\vspace{-1.3em}
		\includegraphics[width=1.25\linewidth]{img/linear4.png}	
		\caption{XOR solvable with, e.g., ReLU (\url{http://playground.tensorflow.org})}
	\end{figure}	
\end{frame}


\begin{frame}{XOR example in super-simplified sentiment classification}
	\begin{figure}
		\vspace{-1.3em}
		\includegraphics[width=1.2\linewidth]{img/xor1.pdf}	
		\caption{$V = \{\text{not}, \text{bad}, \text{good}\}$, binary features $\in \{0, 1\}$}
	\end{figure}	
\end{frame}






\begin{frame}{Multi-layer perceptron (MLP) aka.\ feed-forward network}
	\vspace{-1em}
	$$
	f(\bm{x}) = \sigma \left( g \left(
	\bm{x} \bm{W^1} + \bm{b^1}
	\right)
	\bm{W^2} + \bm{b^2} \right)
	$$
	
	\begin{figure}
		\begin{tikzpicture}	
			%\node (a1) [draw, circle, inner sep=0pt, minimum width=0.75cm, fill=green!20] {$a_1$};
			\node (x) [constant] {$\bm{x}$};
			\node (w) [param, below of=x] {$\bm{W^1}$};
			\node (b) [param, below of=w] {$\bm{b^1}$};
			
			\node (f1) [neuron, right of=w, xshift=1.5cm] {$\bm{x} \bm{W^1} + \bm{b^1}$};
			
			\node (g) [neuron, right of=f1, xshift=1.5cm] {$g$};
			\node (f2) [neuron, right of=g, xshift=1.5cm] {$\bm{h^1} \bm{W^2} + \bm{b^2}$};
			
			\node (w2) [param, below of=f2, xshift=-1.5cm, yshift=0cm] {$\bm{W^2}$};
			\node (b2) [param, below of=f2, xshift=-0.5cm, yshift=-0.5cm] {$\bm{b^2}$};
			
			\node (sigma) [neuron, right of=f2, xshift=2cm] {$\sigma$ or softmax};
			
			\node (l) [neuron, right of=sigma, xshift=1.5cm] {$L$};
			\node (y) [constant, below of=sigma, xshift=1.5cm] {$\bm{y}$};
			
			\begin{scope}[thick, black, ->, >=latex]
				\draw (x) -- (f1);
				\draw (w) -- (f1);
				\draw (b) -- (f1);
				\draw (f1) -- (g);
				\draw (g) -- (f2);
				\draw (f2) -- (sigma);
				\draw (sigma) -- (l);
				\draw (w2) -- (f2);
				\draw (b2) -- (f2);
				\draw (y) -- (l);
			\end{scope}	
		\end{tikzpicture}
		\caption{Computational graph; green boxes are trainable parameters, gray are constant inputs}
	\end{figure}	
	
\end{frame}




\section{Language modeling}



\subsection{`Classical' language models}

\begin{frame}{Goal of language modeling}
	
	Assign a probability to sentences in a language
	
	\begin{example}
		``What is the probability of seeing the sentence \emph{the lazy dog barked loudly}?"
	\end{example}
	
	Assigns a probability for the likelihood of given word (or a sequence of words) to follow a sequence of words
	
	\begin{example}
		``What is the probability of seeing the word \emph{barked} after the seeing sequence \emph{the lazy dog}?
	\end{example}
	
\end{frame}

\begin{frame}{Language models formally}
	Sequence of words $w_{1:n} = w_1 w_2 w_3 \ldots w_n$ estimate
	$$
	\Pr(w_{1:n}) = \Pr(w_1, w_2, \ldots, w_n)
	$$
	\begin{block}{Note: We misuse notation and usually omit the RVs}
		$\Pr(W_1 = w_1, W_1 = w_2, \ldots, W_n = w_n)$
	\end{block}
	
	We \emph{factorize} the joint probability into a product
	\begin{itemize}
		\item One factorization is very useful: left-to-right
	\end{itemize}
	$$
	\begin{aligned}
	\Pr(w_{1:n}) &= \Pr(w_1 | \text{<s>}) \Pr (w_2 | \text{<s>}, w_1) \Pr(w_3 | \text{<s>}, w_1, w_2) \cdots\\
	&\cdots \Pr(w_n | \text{<s>}, w_1, w_2, \ldots, w_{n-1})
	\end{aligned}
	$$
	
\end{frame}

\begin{frame}{Simplifications in `classical' language models}
	Despite factorization, the last term of
	$
\Pr(w_{1:n}) = \Pr(w_1 | \text{<s>}) \Pr (w_2 | \text{<s>}, w_1) \Pr(w_3 | \text{<s>}, w_1, w_2) \cdots \Pr(w_n | \text{<s>}, w_1, w_2, \ldots, w_{n-1})
	$
	still depends on all the previous words of the sequence
	
	\begin{block}{$k$-th order markov-assumption}
		The next word depends only on the last $k$ words
		$$
		\Pr(w_{i} | w_{1:i-1}) \approx \Pr (w_{i} | w_{i-k:i-1}) \qquad \text{\footnotesize{(inclusive indexing!)}}
		$$
	\end{block}
\end{frame}

\begin{frame}{Estimating probabilities in `classical' language models}
	Maximum Likelihood Estimation (aka.\ counting and dividing)
	$$
	\hat{P}_{\text{MLE}}(W_i = w | w_{i - k:i - 1}) = \frac{\#(w_{i-k} \quad w_{i-k+1} \quad \ldots \quad w_{i-1} \quad w)}{\#(w_{i-k} \quad w_{i-k+1} \quad \ldots \quad w_{i-1})}
	$$
	
\end{frame}


\begin{frame}{Evaluating language models: Perplexity}
	Recall: Trained LM tells us probability of `sentence' $s$: $\Pr(s)$
	
	\pause
	Let's have $n$ sentences in a test corpus, each of them has a uniform probability of appearing: $\frac{1}{n}$
	
	\pause
	Then the \textbf{cross-entropy}  of our model is
	$$
	\sum_{i = 1}^{n} \frac{1}{n} \log (\frac{1}{\Pr(s_i)}) = \pause
	\frac{1}{n} \sum_{i = 1}^{n} \log (\frac{1}{\Pr(s_i)}) = \pause
	- \frac{1}{n} \sum_{i = 1}^{n} \log \Pr(s_i)
	$$
	\pause \begin{block}{Perplexity of LM}
		$$
		2^{\text{cross-entropy}} = 
		2^{\left(- \frac{1}{n} \sum_{i = 1}^{n} \log \Pr(s_i)\right)}
		$$
	\end{block}
	
\end{frame}

\begin{frame}{Shortcomings of $n$-gram language models}
	
	\pause
	Long-range dependencies
	\begin{itemize}
		\item To capture a dependency between the next word and the word 10 positions in the past, we need to see a relevant 11-gram in the text
	\end{itemize}
	
	\pause
	Lack of generalization across contexts
	\begin{itemize}
		\item Having observed \emph{black car} and \emph{blue car} does not influence our estimates of the event \emph{red car} if we haven't see it before
	\end{itemize}
	
\begin{tikzpicture}[overlay, remember picture] 
	\node at (current page.north east)[ref] {\fullcite[p.~108]{Goldberg.2017} \par};
\end{tikzpicture}


\end{frame}

\subsection{Neural language models}

\begin{frame}{Neural LMs}
	
	Let's build a neural network
	\begin{itemize}
		\item Input: a $k$-gram of words $w_{1:k}$
		\item Desired output: a probability distribution over the vocabulary $V$ for the next word $w_{k+1}$
	\end{itemize}
	
\end{frame}

\begin{frame}{Embedding layer}
	
	If the input are symbolic \textbf{categorical features}
	\begin{itemize}
		\item e.g., words from a closed vocabulary
	\end{itemize}
	it is common to associate each possible feature value
	\begin{itemize}
		\item i.e., each word in the vocabulary
	\end{itemize}
	with a $d$-dimensional vector for some $d$
	
	\bigskip
	
	These vectors are also \emph{parameters} of the model, and are trained jointly with the other parameters
	
\end{frame}

\begin{frame}{Embedding layer: Lookup operation}
	
	The mapping from a symbolic feature values such as \texttt{word-number-48} to $d$-dimensional vectors is performed by an embedding layer (a lookup layer)
	
	The parameters in an embedding layer is a matrix $\bm{E}^{\abs{V} \times d}$, each row corresponds to a different word in the vocabulary
	
	The lookup operation is then indexing $v(w)$, e.g.,
	$$v(w) = v_{48} = \bm{E}_{[48,:]}$$
	
	If the symbolic feature is encoded as a one-hot vector $\bm{x}$, the lookup operation can be implemented as the multiplication $\bm{x} \bm{E}$
	
\end{frame}


\begin{frame}{Network concatenating 3 words as embeddings ($d_w = 50$)}
	\vspace{-1em}
	%	$$	f(\bm{x}) = g \left(	\bm{x} \bm{W^1} + \bm{b^1}	\right)	\bm{W^2} + \bm{b^2}	$$
	\begin{figure}
		\begin{tikzpicture}	
			%\node (a1) [draw, circle, inner sep=0pt, minimum width=0.75cm, fill=green!20] {$a_1$};
			
			\node (the) {the};
			\node (black) [below of=the, yshift=-0.5cm]{black};
			\node (dog) [below of=black, yshift=-0.5cm]{dog};
			\node (e) [param, below of=dog, yshift=-0.1cm] {$\bm{E}$};
			
			\node (lookup1) [neuron, right of=the, xshift=1.5cm] {lookup};
			\node (lookup2) [neuron, below of=lookup1, yshift=-0.5cm] {lookup};
			\node (lookup3) [neuron, below of=lookup2, yshift=-0.5cm] {lookup};			
			\node (concat) [neuron, right of=lookup1, xshift=1cm, yshift=-1cm] {concat};
			
			
			%			\node (x) [constant, right of=concat, xshift=1cm] {$\bm{x}$};
			\node (w) [param, below of=concat, yshift=-0.2cm] {$\bm{W^1}$};
			\node (b) [param, below of=w] {$\bm{b^1}$};
			
			\node (f1) [neuron, right of=w, xshift=1.5cm] {$g(\bm{x} \bm{W^1} + \bm{b^1})$};
			
			\node (f2) [neuron, right of=f1, xshift=3cm] {$\softmax(\bm{h^1} \bm{W^2} + \bm{b^2})$};
			
			\node (w2) [param, below of=f2, xshift=-1.5cm, yshift=0cm] {$\bm{W^2}$};
			\node (b2) [param, below of=f2, xshift=-0.5cm, yshift=-0.5cm] {$\bm{b^2}$};
			
			\node (l) [neuron, right of=f2, xshift=2cm] {$L$};
			\node (y) [constant, below of=f2, xshift=1.5cm] {$\bm{y}$};
			
			\begin{scope}[thick, black, ->, >=latex]
				\draw (the) -- (lookup1);
				\draw (black) -- (lookup2);
				\draw (dog) -- (lookup3);
				\draw (e) -- (lookup1);
				\draw (e) -- (lookup2);
				\draw (e) -- (lookup3);
				
				\draw (lookup1) -- (concat);
				\draw (lookup2) -- (concat);
				\draw (lookup3) -- (concat);								
				
				\draw (concat) -- (f1) node [midway, above] {$\bm{x}$};
				\draw (w) -- (f1);
				\draw (b) -- (f1);
				\draw (f1) -- (f2);
				\draw (f2) -- (l);
				\draw (w2) -- (f2);
				\draw (b2) -- (f2);
				\draw (y) -- (l);
			\end{scope}	
		\end{tikzpicture}
	\end{figure}	
	
	Each word $\in \mathbb{R}^{\abs{V}}$ (one hot), 
	$\bm{E} \in \mathbb{R}^{\abs{V} \times 50}$, each lookup output $\in \mathbb{R}^{50}$, concat output $\bm{x} \in \mathbb{R}^{150}$
	
	
\end{frame}


\begin{frame}{Neural LMs}
	
	Let's build a neural network
	\begin{itemize}
		\item Input: a $k$-gram of words $w_{1:k}$
		\item Desired output: a probability distribution over the vocabulary $V$ for the next word $w_{k+1}$
	\end{itemize}
	
	Each input word $w_k$ is associated with an embedding vector $v(w) \in \mathbb{R}^{d_w}$ ($d_w$ --- word embedding dimensionality)
	
	Input vector $\bm{x}$ is a concatenation of $k$ words
	$$
	\bm{x} = \left[ v(w_1); v(w_2); \ldots; v(w_k) \right]
	$$
	
\end{frame}

\begin{frame}{Neural LMs}
	
	MLP with one (or more) hidden layers
	
	$$
	\begin{aligned}
		v(w) &= \bm{E}_{w,:} \\
		\bm{x} &= \left[ v(w_1); v(w_2); \ldots; v(w_k) \right] \\
		\bm{h} &= g(\bm{x} \bm{W^1} + \bm{b^1}) \\
		\hat{\bm{y}} = \Pr(W_i | w_{1:k}) &= \softmax (\bm{h} \bm{W^2} + \bm{b^2})
	\end{aligned}
	$$
	
	Output dimension: $\hat{\bm{y}} \in \mathbb{R}^{\abs{V}}$
	
\end{frame}

\begin{frame}{Training neural LMs}
	
	Where to get training examples?
	
	Training examples are simply word $k$-grams from an unlabeled corpus
	\begin{itemize}
		\item Identities of the first $k - 1$ words are used as features
		\item The last word is used as the target label for the classification
	\end{itemize}
	
	The model is trained using cross-entropy loss
\end{frame}

\begin{frame}{Some advantages and limitations of neural LMs}
	
	$\approx$ linear increase in parameters with $k + 1$ (better than `classical' LMs) but
	\begin{itemize}
		\item The size of the output vocabulary affects the computation time
		\item The softmax at the output layer requires an expensive matrix-vector multiplication with the matrix $\bm{W^2} \in \mathbb{R}^{d_{\text{hid} \times \abs{V}}}$, followed by $\abs{V}$ exponentiations
	\end{itemize}
	
	Solutions: Hierarchical softmax, noise-contrastive estimation
	
\end{frame}

\begin{frame}{Generating text with language models}
	
	We can generate (``sample") random sentences from the model according to their probability
	
	\begin{enumerate}
		\item Predict a probability distribution over the vocabulary conditioned on the start symbol <s>
		\item Draw the first word from the predicted distribution
		\item Predict a probability distribution over the vocabulary conditioned on the start symbol and the first word
		\item Draw the second word from the predicted distribution
		\item Repeat until generated \emph{end-of-sentence} symbol </s> (or <EOS>)
	\end{enumerate}
	
	
\end{frame}

\begin{frame}{Learned word representations as a by-product}
	\vspace{-1em}
	%	$$	f(\bm{x}) = g \left(	\bm{x} \bm{W^1} + \bm{b^1}	\right)	\bm{W^2} + \bm{b^2}	$$
	\begin{figure}
	\begin{tikzpicture}	
		%\node (a1) [draw, circle, inner sep=0pt, minimum width=0.75cm, fill=green!20] {$a_1$};
		
		\node (the) {the};
		\node (black) [below of=the, yshift=-0.5cm]{black};
		\node (dog) [below of=black, yshift=-0.5cm]{dog};
		\node (e) [param, below of=dog, yshift=-0.1cm] {$\bm{E}$};
		
		\node (lookup1) [neuron, right of=the, xshift=1.5cm] {lookup};
		\node (lookup2) [neuron, below of=lookup1, yshift=-0.5cm] {lookup};
		\node (lookup3) [neuron, below of=lookup2, yshift=-0.5cm] {lookup};			
		\node (concat) [neuron, right of=lookup1, xshift=1cm, yshift=-1cm] {concat};
		
		
		%			\node (x) [constant, right of=concat, xshift=1cm] {$\bm{x}$};
		\node (w) [param, below of=concat, yshift=-0.2cm] {$\bm{W^1}$};
		\node (b) [param, below of=w] {$\bm{b^1}$};
		
		\node (f1) [neuron, right of=w, xshift=1.5cm] {$g(\bm{x} \bm{W^1} + \bm{b^1})$};
		
		\node (f2) [neuron, right of=f1, xshift=3cm] {$\softmax(\bm{h^1} \bm{W^2} + \bm{b^2})$};
		
		\node (w2) [param, below of=f2, xshift=-1.5cm, yshift=0cm] {$\bm{W^2}$};
		\node (b2) [param, below of=f2, xshift=-0.5cm, yshift=-0.5cm] {$\bm{b^2}$};
		
		\node (l) [neuron, right of=f2, xshift=2cm] {$L$};
		\node (y) [constant, below of=f2, xshift=1.5cm] {$\bm{y}$};
		
		\begin{scope}[thick, black, ->, >=latex]
			\draw (the) -- (lookup1);
			\draw (black) -- (lookup2);
			\draw (dog) -- (lookup3);
			\draw (e) -- (lookup1);
			\draw (e) -- (lookup2);
			\draw (e) -- (lookup3);
			
			\draw (lookup1) -- (concat);
			\draw (lookup2) -- (concat);
			\draw (lookup3) -- (concat);								
			
			\draw (concat) -- (f1) node [midway, above] {$\bm{x}$};
			\draw (w) -- (f1);
			\draw (b) -- (f1);
			\draw (f1) -- (f2);
			\draw (f2) -- (l);
			\draw (w2) -- (f2);
			\draw (b2) -- (f2);
			\draw (y) -- (l);
		\end{scope}	
	\end{tikzpicture}
\end{figure}	
	
	Each row of $\bm{E}$ learns a word representation
	
	
\end{frame}

\section{Word embeddings}

\begin{frame}{Word embeddings as pre-trained word representation}
	
	Option A: We can initialize the embeddings matrix $\bm{E}$ randomly and learn during our supervised task
	
	Option B: Use pre-trained word embeddings from task for which we have a lot of data
	
	\begin{itemize}
		\item Use self-supervised learning (create labeled data `for free' using the next word prediction objective)
		\item Learned word embedding matrix plugged into our supervised task
	\end{itemize}
	
	Desired word embeddings properties: `Similar' words have similar embeddings vectors
\end{frame}




\begin{frame}{Take aways}
	
	\begin{itemize}
		\item Language modeling is an essential part of contemporary NLP
		\item Self-supervised models, unlabeled data, next word prediction
		\item Neural language models learn embedding of words
	\end{itemize}
	
\end{frame}





\begin{frame}{License and credits}

	\begin{columns}
		\begin{column}{0.7\textwidth}
			Licensed under Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)
		\end{column}
		\begin{column}{0.2\textwidth}
			\includegraphics[width=0.9\linewidth]{img/cc-by-sa-icon.pdf}
		\end{column}
	\end{columns}
	
	\bigskip
	
	Credits
	
	\begin{scriptsize}
		
		Ivan Habernal
		
		Content from ACL Anthology papers licensed under CC-BY \url{https://www.aclweb.org/anthology}
		
	\end{scriptsize}
	
\end{frame}


\appendix

\section{Appendix: Probability refresher}


\begin{frame}{Probability refresher 1}
	
	\begin{block}{Categorical random variables}
		For example, the first word in a sentence
		
		$W_1 \in \{ \textrm{the, be, to, of, and, } \ldots \}$, we assume a fixed vocabulary
	\end{block}
	
	\pause
	
	\begin{block}{Probability distribution over random variables}
		For example, probability of \emph{`the'} at position 1
		
		$\Pr(W_1 = w_1) = \Pr(W_1 = \text{the}) = 0.00024$
		
		\pause
		
		Notation shortcuts: $\Pr(W_1 = w_1) \to P(W_1), P(\text{the})$, etc.
	\end{block}
	
\end{frame}


\begin{frame}{Probability refresher 2}
	
	
	\begin{block}{Joint probability}
		For example, probability of \emph{`the'} at position 1 and \emph{`cat'} at position 2
		
		$\Pr(W_1 = \text{the} \cap W_2 = \text{cat}) = 0.0000074$
		
		\pause
		
		Notation shortcuts: $P(W_1, W_2) = P(W_2, W_1)$
	\end{block}
	
	\pause
	
	\begin{block}{Conditional probability}
		For example, probability of \emph{`cat'} at position 2, \textbf{given} \emph{`the'} at position 1
		
		$\Pr(W_2 = \text{cat} | W_1 = \text{the} ) = \frac{P(W_1, W_2)}{P(W_1)}$
	\end{block}
	
\end{frame}

\begin{frame}{Probability refresher 3}
	
	\begin{block}{Independence}
		Two random variables $X, Y$ are \textbf{independent} if and only if
		
		$P(X, Y) = P(X) \cdot P(Y)$
	\end{block}
	
	\pause
	
	\begin{block}{Conditional independence}
		Two random variables $X, Y$ are \textbf{conditionally independent} given $Z$ if and only if
		
		$P(X, Y | Z) = P(X|Z) \cdot P(Y|Z)$
	\end{block}
	
\end{frame}



\end{document}

